{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 13:10:03.653427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/gpflow/experimental/utils.py:43: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  f\"You're calling {name} which is considered *experimental*.\"\n",
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/gpflow/experimental/utils.py:43: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  f\"You're calling {name} which is considered *experimental*.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0. Expected: 2.7.0\n",
      "TensorFlow Probability version: 0.18.0. Expected: 0.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 13:10:26.159966: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from wrapper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAYcMR698j6J"
   },
   "source": [
    "# Experiment II: 2D Spatial Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHthi3uPKLkr"
   },
   "source": [
    "### Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KDl4Di8lKIgm"
   },
   "outputs": [],
   "source": [
    "# Optimization configs. \n",
    "# Consider reduce below parameters / set to `False` if MCMC is taking too long:\n",
    "# mcmc_num_steps, mcmc_burnin, mcmc_nchain, mcmc_initialize_from_map.\n",
    "map_step_size=5e-4   # @param\n",
    "map_num_steps=10_000  # @param\n",
    "\n",
    "mcmc_step_size=1e-4 # @param\n",
    "mcmc_num_steps=1000 # @param\n",
    "\n",
    "mcmc_nchain=10 # @param\n",
    "mcmc_burnin=2_500 # @param\n",
    "bne_mcmc_initialize_from_map=\"True\" # @param [\"False\", \"True\"]\n",
    "\n",
    "bne_mcmc_initialize_from_map = eval(bne_mcmc_initialize_from_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AMIJk1OdKIj7"
   },
   "outputs": [],
   "source": [
    "# BMA parameters.\n",
    "y_noise_std = 0.01  # Note: Changed from 0.1 # @param\n",
    "bma_gp_lengthscale = .05 # @param\n",
    "bma_gp_l2_regularizer = 0.1 # @param\n",
    "\n",
    "bma_n_samples_train = 100 # @param\n",
    "bma_n_samples_eval = 250 # @param\n",
    "bma_n_samples_test = 250 # @param\n",
    "bma_seed = 0 # @param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9iyxQiCJKIn7"
   },
   "outputs": [],
   "source": [
    "# BNE parameters.\n",
    "bne_gp_lengthscale = .05 # 5. # @param\n",
    "bne_gp_l2_regularizer = 1. # 15 # @param\n",
    "bne_variance_prior_mean = -2.5 # @param\n",
    "bne_skewness_prior_mean = -2.5 # @param\n",
    "bne_seed = 0 # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training/prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(55, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(84421, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred longitude max and min -69.93 -73.5\n",
      "pred latitude max and min 44.3 40.6\n",
      "train longitude max and min -70.023598 -73.443056\n",
      "train latitude max and min 44.107524 40.74529\n"
     ]
    }
   ],
   "source": [
    "training_eastMA = pd.read_csv('./data/training_dataset/training_eastMA.csv')\n",
    "training_eastMA_noMI = training_eastMA[:51]\n",
    "training_eastMA_folds = pd.read_csv('./data/training_dataset/training_eastMA_folds.csv')\n",
    "base_model_predictions_eastMA = pd.read_csv('./data/prediction_dataset/base_model_predictions_eastMA.csv')\n",
    "display(training_eastMA.shape, training_eastMA_folds.shape, base_model_predictions_eastMA.shape)\n",
    "print(\"pred longitude max and min\", base_model_predictions_eastMA[\"lon\"].max(),base_model_predictions_eastMA[\"lon\"].min())\n",
    "print(\"pred latitude max and min\", base_model_predictions_eastMA[\"lat\"].max(),base_model_predictions_eastMA[\"lat\"].min())\n",
    "#list(base_model_predictions_eastMA.columns)\n",
    "print(\"train longitude max and min\", training_eastMA[\"lon\"].max(),training_eastMA[\"lon\"].min())\n",
    "print(\"train latitude max and min\", training_eastMA[\"lat\"].max(),training_eastMA[\"lat\"].min())\n",
    "\n",
    "\n",
    "training51= pd.read_csv('./data/training_dataset/training51.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 center and scale:  [-72.185104  42.680347] [3.5699997 3.7000008]\n"
     ]
    }
   ],
   "source": [
    "# standardize\n",
    "X_train1 = np.asarray(training_eastMA_noMI[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "X_test1 = np.asarray(base_model_predictions_eastMA[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "X_valid = np.concatenate((X_train1, X_test1), axis=0)\n",
    "X_centr = np.mean(X_valid, axis=0)\n",
    "X_scale = np.max(X_valid, axis=0) - np.min(X_valid, axis=0)\n",
    "\n",
    "X_train1 = (X_train1 - X_centr) / X_scale\n",
    "X_test1 = (X_test1 - X_centr) / X_scale\n",
    "\n",
    "Y_train = np.expand_dims(training_eastMA_noMI[\"aqs\"], 1).astype(np.float32)\n",
    "#Y_test = np.expand_dims(base_model_predictions_eastMA[\"pred_av\"], 1).astype(np.float32)\n",
    "\n",
    "print(\"2011 center and scale: \", X_centr, X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([51, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([84421, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_names = [\"pred_av\", \"pred_gs\", \"pred_caces\"]\n",
    "base_preds_train = tf.stack([training_eastMA_noMI[base_model_name].astype(np.float32) for base_model_name in base_model_names], axis=-1)\n",
    "base_preds_test = tf.stack([base_model_predictions_eastMA[base_model_name].astype(np.float32) for base_model_name in base_model_names], axis=-1)\n",
    "#base_preds_test\n",
    "display(base_preds_train.shape, base_preds_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Model Averaging\n",
    "\n",
    "A Bayesian ensemble model where ensemble weights $w_k's$ are parameterized by Gaussian process priors:\n",
    "\n",
    "$y \\sim N(\\mu(x), \\sigma^2)$ \n",
    "\n",
    "$\\mu(x) = \\sum_{k=1}^K w_k(x) * m_k(x) \\quad$  where $\\{m_k\\}_{k=1}^K$ are base model predictions.\n",
    "\n",
    "$w(x) = softmax(f(x)) \\qquad\\;\\;\\;$ where $w=[w_1, \\dots, w_K]$ and $f=[f_1, \\dots, f_K]$\n",
    "\n",
    "$f \\stackrel{i.i.d.}{\\sim} GaussianProcess(0, k)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble into configs.\n",
    "bma_model_config = DEFAULT_GP_CONFIG.copy()\n",
    "map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "bma_model_config.update(dict(lengthscale=bma_gp_lengthscale,\n",
    "                             l2_regularizer=bma_gp_l2_regularizer,\n",
    "                             y_noise_std=y_noise_std))\n",
    "\n",
    "map_config.update(dict(learning_rate=map_step_size,\n",
    "                       num_steps=map_num_steps))\n",
    "\n",
    "mcmc_config.update(dict(step_size=mcmc_step_size, \n",
    "                        num_steps=mcmc_num_steps,\n",
    "                       burnin=mcmc_burnin,\n",
    "                       nchain=mcmc_nchain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 10 fold Cross Validation RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer OrthogonalRandomFeatures is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t298387.0625...115085.359375...107380.96875...104392.9375...102979.875...100910.546875...100026.5390625...99396.78125...97088.71875...96693.546875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6906112432479858\n",
      "Running MAP:\t400250.5...147092.484375...132666.96875...125622.5234375...120866.359375...119381.1640625...118420.5078125...116862.796875...116063.5234375...115449.2109375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6670030951499939\n",
      "Running MAP:\t404433.375...138287.171875...125506.484375...120236.640625...116915.03125...115240.6796875...114216.03125...113518.546875...112961.1171875...112534.890625...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7566648721694946\n",
      "Running MAP:\t362267.125...135549.671875...122294.9765625...118344.421875...116712.0703125...115914.1328125...115478.1171875...115205.0546875...115034.453125...114924.6484375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7093912959098816\n",
      "Running MAP:\t283982.90625...105229.71875...94491.40625...90671.640625...88751.4765625...87855.859375...87330.75...86969.6875...86665.53125...86363.4921875...Done.\n",
      "Running MCMC:\t"
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_bma = []\n",
    "\n",
    "for fold_id in range(1, 11):\n",
    "    # prepare cross-validation data\n",
    "    X_te = X_train1[training51.index[training51[\"fold\"] == fold_id]]\n",
    "    X_tr = X_train1[training51.index[training51[\"fold\"] != fold_id]]\n",
    "    Y_te = Y_train[training51.index[training51[\"fold\"] == fold_id]]\n",
    "    Y_tr = Y_train[training51.index[training51[\"fold\"] != fold_id]]\n",
    "\n",
    "    base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "    base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "    # build model & run MCMC\n",
    "    bma_prior, bma_gp_config = bma_dist(X_tr, \n",
    "                                    base_preds_tr, \n",
    "                                    **bma_model_config)\n",
    "\n",
    "    bma_model_config.update(bma_gp_config)\n",
    "\n",
    "\n",
    "    bma_gp_w_samples = run_posterior_inference(model_dist=bma_prior, \n",
    "                                           model_config=bma_model_config,\n",
    "                                           Y=Y_tr, \n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config)\n",
    "\n",
    "\n",
    "    bma_joint_samples = make_bma_samples(X_te, None, base_preds_te, \n",
    "                                     bma_weight_samples=bma_gp_w_samples[0],\n",
    "                                     bma_model_config=bma_model_config,\n",
    "                                     n_samples=bma_n_samples_eval, \n",
    "                                     seed=bne_seed,\n",
    "                                     y_samples_only=False)\n",
    "\n",
    "    y_pred = bma_joint_samples['y']\n",
    "    y_pred = tf.reduce_mean(y_pred, axis=0)\n",
    "\n",
    "    rmse_bma.append(rmse(Y_te, y_pred))\n",
    "rmse_bma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.14731"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rmse_bma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('gp_weights', ()), ('y', ('gp_weights',)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bma_prior, bma_gp_config = bma_dist(X_train1, \n",
    "                                    base_preds_train, \n",
    "                                    **bma_model_config)\n",
    "\n",
    "bma_model_config.update(bma_gp_config)\n",
    "\n",
    "# Check if the model graph is specified correctly.\n",
    "bma_prior.resolve_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t428139.1875...217879.703125...211721.484375...208145.953125...206213.78125...203751.0625...201055.578125...199206.984375...197395.78125...195403.625...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.850577712059021\n"
     ]
    }
   ],
   "source": [
    "# bma_gp_w_samples, chain_samples, sampler_stat = run_posterior_inference(model_dist=bma_prior, \n",
    "#                                            model_config=bma_model_config,\n",
    "#                                            Y=Y_train, \n",
    "#                                            map_config=map_config,\n",
    "#                                            mcmc_config=mcmc_config)\n",
    "\n",
    "# Above the debug mode\n",
    "\n",
    "bma_gp_w_samples = run_posterior_inference(model_dist=bma_prior, \n",
    "                                           model_config=bma_model_config,\n",
    "                                           Y=Y_train, \n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config)\n",
    "\n",
    "\n",
    "bma_joint_samples = make_bma_samples(X_test1, None, base_preds_test, \n",
    "                                     bma_weight_samples=bma_gp_w_samples[0],\n",
    "                                     bma_model_config=bma_model_config,\n",
    "                                     n_samples=bma_n_samples_eval, \n",
    "                                     seed=bne_seed,\n",
    "                                     y_samples_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check with Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE:  1.6748887\n"
     ]
    }
   ],
   "source": [
    "#np.mean((means_pred - means_true)**2) / np.var(means_true)\n",
    "# mse = tf.reduce_mean((means_train_mcmc-Y_train_mcmc)** 2)/ np.var(Y_train_mcmc)\n",
    "# mse\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def rmse(y_obs, y_pred):\n",
    "    return np.sqrt(np.mean((y_obs - y_pred) ** 2))\n",
    "\n",
    "\n",
    "reg = LinearRegression().fit(X_train1, Y_train)\n",
    "y_pred = reg.predict(X_train1)\n",
    "rmse_lr = mean_squared_error(y_true=Y_train, y_pred=y_pred, squared=False)\n",
    "print(\"Linear Regression RMSE: \", rmse_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute sample stats.\n",
    "# sample_mean = tf.reduce_mean(means_train_mcmc, axis=[0, 1])\n",
    "# # ==> approx equal to weights\n",
    "\n",
    "# sample_var = tf.reduce_mean(\n",
    "#     tf.math.squared_difference(chain_samples, sample_mean),\n",
    "#     axis=[0, 1])\n",
    "# ==> less than 1\n",
    "\n",
    "# def rmse(y_obs, y_pred):\n",
    "#     \"\"\"Computes root mean square error.\"\"\"\n",
    "#     return np.sqrt(np.mean((y_obs.squeeze() - y_pred.squeeze()) ** 2))\n",
    "# rmse(Y_train_mcmc, means_train_mcmc)\n",
    "\n",
    "# samples = samples[:num_sample]\n",
    "# means_pred = np.mean(samples, axis=0)\n",
    "# stds_pred = np.std(samples, axis=0)\n",
    "# mse_ind = np.mean(\n",
    "#     (means_pred[ind_ids] - means_true[ind_ids])**2) / np.var(means_true[ind_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for BAE/BNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer OrthogonalRandomFeatures is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    }
   ],
   "source": [
    "# Construct data from BMA samples, shapes (num_samples * num_data, ...)\n",
    "means_train_mcmc, X_train_mcmc, Y_train_mcmc = make_bma_samples(\n",
    "    X_train1, Y_train, base_preds_train, \n",
    "    bma_weight_samples=bma_gp_w_samples[0],\n",
    "    bma_model_config=bma_model_config,\n",
    "    n_samples=bma_n_samples_train,\n",
    "    seed=bma_seed, \n",
    "    prepare_mcmc_training=True)\n",
    "\n",
    "# Mean samples based on test data, shape (num_samples, num_data, num_output).\n",
    "# It is used to generate final examples in `make_bne_samples()`.\n",
    "means_test_mcmc = make_bma_samples(\n",
    "    X_test1, None, base_preds_test, \n",
    "    bma_weight_samples=bma_gp_w_samples[0],\n",
    "    bma_model_config=bma_model_config,\n",
    "    n_samples=bma_n_samples_test,\n",
    "    seed=bma_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spcv_df = tf.repeat(training51[\"fold\"], bma_n_samples_train, axis=0)\n",
    "\n",
    "# fold_id=2\n",
    "\n",
    "# X_te = X_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "# X_tr = X_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "# Y_te = Y_train_mcmc[spcv_df==fold_id]\n",
    "# Y_tr = Y_train_mcmc[spcv_df!=fold_id]\n",
    "# means_tr_mcmc = means_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "# means_te_mcmc = means_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "\n",
    "# base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "# base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "# # Construct posterior sampler.\n",
    "# # Construct posterior sampler.\n",
    "# bne_prior, bne_gp_config = bne_model_dist(\n",
    "#     inputs=X_tr,\n",
    "#     mean_preds=means_tr_mcmc,\n",
    "#     **bne_model_config)\n",
    "\n",
    "# bne_model_config.update(bne_gp_config)\n",
    "\n",
    "\n",
    "# # Estimates GP weight posterior using MCMC.\n",
    "# bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "#                                            model_config=bne_gp_config,\n",
    "#                                            Y=Y_tr,\n",
    "#                                            map_config=map_config,\n",
    "#                                            mcmc_config=mcmc_config,\n",
    "#                                            initialize_from_map=True)\n",
    "# # Generates the posterior sample for all model parameters. \n",
    "# bne_joint_samples = make_bne_samples(X_te,\n",
    "#                                      mean_preds=means_te_mcmc,\n",
    "#                                      bne_model_config=bne_model_config,\n",
    "#                                      bne_weight_samples=bne_gp_w_samples[0],\n",
    "#                                      seed=bne_seed)\n",
    "# y_pred = bne_joint_samples['y']\n",
    "# means_pred = np.mean(y_pred, axis=0)\n",
    "# rmse_bne = np.sqrt(np.mean((means_pred - Y_te)**2))\n",
    "# rmse_bne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Additive Ensemble\n",
    "\n",
    "Given $\\mu(x)$ the posterior of a Bayesian ensemble model, the Bayesian Additive Ensemble is defined as:    \n",
    "\n",
    "$y \\sim N(\\mu(x) + r(x), \\sigma^2)$\n",
    "\n",
    "$r \\sim GaussianProcess(0, k)$\n",
    "\n",
    "The additive ensemble $r(x)$ services two purposes: \n",
    "\n",
    "1. Mitigates systematic bias in model prediction; \n",
    "2. Quantifies the model's epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # BNE GP Configs.\n",
    "# # lengthscale = 1. # @param\n",
    "# # l2_regularizer = 10. # @param\n",
    "\n",
    "# BNE model configs. \n",
    "# If estimate_mean=False, only estimates a constant variance on top of the \n",
    "# original model.\n",
    "estimate_mean = \"True\" # @param [\"True\", \"False\"]\n",
    "variance_prior_mean=0. # @param\n",
    "# # MAP and MCMC configs\n",
    "# map_step_size=0.1 # @param\n",
    "# map_num_steps=10_000 # @param\n",
    "\n",
    "# mcmc_step_size=1e-2 # @param\n",
    "# mcmc_num_steps=10_000 # @param\n",
    "\n",
    "bne_gp_config = DEFAULT_GP_CONFIG.copy()\n",
    "bne_model_config = DEFAULT_BNE_CONFIG.copy()\n",
    "\n",
    "map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "\n",
    "bne_gp_config.update(dict(lengthscale=bne_gp_lengthscale, \n",
    "                          l2_regularizer=bne_gp_l2_regularizer))\n",
    "bne_model_config.update(dict(estimate_mean=eval(estimate_mean),\n",
    "                             variance_prior_mean=variance_prior_mean,\n",
    "                             **bne_gp_config))\n",
    "\n",
    "map_config.update(dict(learning_rate=map_step_size,\n",
    "                       num_steps=map_num_steps))\n",
    "mcmc_config.update(dict(step_size=mcmc_step_size, \n",
    "                        num_steps=mcmc_num_steps,\n",
    "                       burnin=mcmc_burnin,\n",
    "                       nchain=mcmc_nchain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t5027.3525390625...4667.4423828125...4667.14892578125...4667.1494140625...4667.1494140625...4667.1494140625...4667.1494140625...4667.1494140625...4667.1494140625...4667.1484375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6501122713088989\n",
      "Running MAP:\t6941.068359375...5982.5302734375...5979.0703125...5979.0546875...5979.0546875...5979.0546875...5979.0546875...5979.0537109375...5979.0537109375...5979.0537109375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7149558067321777\n",
      "Running MAP:\t6899.20703125...6056.43359375...6054.19873046875...6054.1953125...6054.19384765625...6054.19482421875...6054.1943359375...6054.19482421875...6054.19482421875...6054.19482421875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.754821240901947\n",
      "Running MAP:\t6668.78857421875...6144.14501953125...6143.5908203125...6143.58984375...6143.5908203125...6143.5908203125...6143.5908203125...6143.5908203125...6143.5908203125...6143.5908203125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6947862505912781\n",
      "Running MAP:\t5832.90185546875...5519.76708984375...5519.33349609375...5519.3330078125...5519.33203125...5519.33203125...5519.3330078125...5519.33203125...5519.3330078125...5519.3330078125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6257717609405518\n",
      "Running MAP:\t6981.97412109375...6195.37353515625...6193.5361328125...6193.5283203125...6193.52783203125...6193.52783203125...6193.52783203125...6193.52783203125...6193.52880859375...6193.52880859375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7022809982299805\n",
      "Running MAP:\t6704.81298828125...6115.3115234375...6114.14892578125...6114.1474609375...6114.146484375...6114.146484375...6114.146484375...6114.146484375...6114.146484375...6114.146484375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7918791770935059\n",
      "Running MAP:\t6698.78515625...5973.9833984375...5971.62841796875...5971.6064453125...5971.60595703125...5971.6064453125...5971.6064453125...5971.6064453125...5971.6064453125...5971.6064453125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7059708833694458\n",
      "Running MAP:\t6346.10546875...5922.1806640625...5921.4443359375...5921.44287109375...5921.44287109375...5921.44287109375...5921.44287109375...5921.44287109375...5921.44287109375...5921.443359375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6821147203445435\n",
      "10 fold cross validation rmse: [1.3746537, 0.2880368, 0.8399746, 0.9543882, 2.2803862, 0.7404471, 0.69910735, 1.3368235, 1.5638318]\n",
      "mean rmse: 1.1197387\n"
     ]
    }
   ],
   "source": [
    "spcv_df = tf.repeat(training51[\"fold\"], bma_n_samples_train, axis=0)\n",
    "rmse_bae = []\n",
    "for fold_id in range(1,11):\n",
    "    X_te = X_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "    X_tr = X_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "    Y_te = Y_train_mcmc[spcv_df==fold_id]\n",
    "    Y_tr = Y_train_mcmc[spcv_df!=fold_id]\n",
    "    means_tr_mcmc = means_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "    means_te_mcmc = means_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "\n",
    "    base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "    base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "    # Construct posterior sampler.\n",
    "    bne_prior, bne_gp_config = bne_model_dist(\n",
    "        inputs=X_tr,\n",
    "        mean_preds=means_tr_mcmc,\n",
    "        **bne_model_config)\n",
    "\n",
    "    bne_model_config.update(bne_gp_config)\n",
    "\n",
    "\n",
    "    # Estimates GP weight posterior using MCMC.\n",
    "    bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                            model_config=bne_gp_config,\n",
    "                                            Y=Y_tr,\n",
    "                                            map_config=map_config,\n",
    "                                            mcmc_config=mcmc_config,\n",
    "                                            initialize_from_map=True)\n",
    "    # Generates the posterior sample for all model parameters. \n",
    "    bne_joint_samples = make_bne_samples(X_te,\n",
    "                                        mean_preds=means_te_mcmc,\n",
    "                                        bne_model_config=bne_model_config,\n",
    "                                        bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                        seed=bne_seed)\n",
    "    \n",
    "    y_pred = bne_joint_samples['y']\n",
    "    means_pred = np.mean(y_pred, axis=0)\n",
    "    rmse_bae.append(np.sqrt(np.mean((means_pred - Y_te)**2)))\n",
    "print(\"10 fold cross validation rmse:\", rmse_bae)\n",
    "print(\"mean rmse:\", np.mean(rmse_bae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior model graph: (('gp_weights', ()), ('y', ('gp_weights',)))\n"
     ]
    }
   ],
   "source": [
    "# Construct posterior sampler.\n",
    "bne_prior, bne_gp_config = bne_model_dist(\n",
    "    inputs=X_train_mcmc,\n",
    "    mean_preds=means_train_mcmc,\n",
    "    **bne_model_config)\n",
    "\n",
    "bne_model_config.update(bne_gp_config)\n",
    "print(f'prior model graph: {bne_prior.resolve_graph()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t8040.71630859375...7257.259765625...7244.951171875...7239.53125...7238.07568359375...7237.91650390625...7237.91259765625...7237.912109375...7237.912109375...7237.912109375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7726417183876038\n"
     ]
    }
   ],
   "source": [
    "# Estimates GP weight posterior using MCMC.\n",
    "bne_gp_w_samples, chain_samples, sampler_stat = run_posterior_inference(model_dist=bne_prior,\n",
    "                                           model_config=bne_gp_config,\n",
    "                                           Y=Y_train_mcmc,\n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config,\n",
    "                                           initialize_from_map=True)\n",
    "# Generates the posterior sample for all model parameters. \n",
    "bne_joint_samples = make_bne_samples(X_test1,\n",
    "                                     mean_preds=means_test_mcmc,\n",
    "                                     bne_model_config=bne_model_config,\n",
    "                                     bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                     seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_pred_bae = {k: np.mean(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "surface_var_bae = {k: np.var(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "\n",
    "# dealing with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005235664112010045"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentile of NAN\n",
    "np.sum(np.isnan(np.mean(bne_joint_samples['mean_original'], axis=0)))/84421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Nonparametric Ensemble (Variance Only)\n",
    "So far, we are only estimating the mean-component of the model, i.e., we are assuming: \n",
    "\n",
    "$y \\sim Gaussian(m(x), \\sigma^2); \\quad m(x) = GP(0, k)$.\n",
    "\n",
    "By doing so, the model is implicitly assuming the distribution of $y$ is always a symmetric Gaussian distribution with constant mean across space and time. As a result, our model can only quantify model uncertainty (due to lack of data) via the GP prior, but cannot flexibly capture the data uncertainty that is inherent to the empirical distribution of y.\n",
    "\n",
    "To resolve this, we extend the ensemble's outcome distribution $y | f$ by also estimating the higher moments of the data distribution (e.g., variance, skewness, etc) using flexible estimators. Specifically, we specify the outcome distribution family to the [maximum-entropy distribution](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) given the known moments, so the predictive distribution is [minimax](https://arxiv.org/pdf/math/0410076.pdf) and still statistically efficient to estimate.\n",
    "\n",
    "For example, when we want to estimate the first two moments (mean and variance) of the distribution, this leads to a Gaussian distribution with spatio-temporally adaptive variance $\\sigma(x)^2$:\n",
    "\n",
    "$$y \\sim Gaussian(m(x), \\sigma(x)^2); \\quad \\mbox{where} \\quad m \\sim GP(0, k_m), \\sigma \\sim GP(0, k_\\sigma)$$\n",
    "\n",
    "and when we want to estimate the first three moments (mean and variance) of the distribution, this leads to a [Exponentially-modifed Gaussian](https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution) (EMG) distribution with spatio-temporally adaptive variance $\\sigma(x)^2$ and skewness $\\lambda(x)$:\n",
    "\n",
    "$$y \\sim EMG(m(x), \\sigma(x)^2, \\lambda(x)); \\quad \\mbox{where} \\quad m \\sim GP(0, k_m), \\sigma \\sim GP(0, k_\\sigma), \\lambda \\sim GP(0, k_\\lambda)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model & Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior model graph: (('gp_weights', ()), ('y', ('gp_weights',)))\n",
      "Running MAP:\t7446.732421875...7053.1435546875...7051.3798828125...7051.36865234375...7051.36865234375...7051.36865234375...7051.36865234375...7051.36865234375...7051.36865234375...7051.36865234375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6818126440048218\n"
     ]
    }
   ],
   "source": [
    "# Construct posterior sampler.\n",
    "bne_prior, bne_gp_config = bne_model_dist(\n",
    "    inputs=X_train_mcmc,\n",
    "    mean_preds=means_train_mcmc,\n",
    "    **bne_model_config)\n",
    "\n",
    "bne_model_config.update(bne_gp_config)\n",
    "print(f'prior model graph: {bne_prior.resolve_graph()}')\n",
    "\n",
    "# Estimates GP weight posterior using MCMC.\n",
    "bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                           model_config=bne_gp_config,\n",
    "                                           Y=Y_train_mcmc,\n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config,\n",
    "                                           initialize_from_map=True)\n",
    "# Generates the posterior sample for all model parameters. \n",
    "bne_joint_samples = make_bne_samples(X_test1,\n",
    "                                     mean_preds=means_test_mcmc,\n",
    "                                     bne_model_config=bne_model_config,\n",
    "                                     bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                     seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_pred_bne_vo = {k: np.mean(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "surface_var_bne_vo = {k: np.var(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88_VOXlgq9n_"
   },
   "source": [
    "## Bayesian Nonparametric Ensemble (Variance + Skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model & Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior model graph: (('gp_weights', ()), ('y', ('gp_weights',)))\n",
      "Running MAP:\t7174.9990234375...7055.798828125...7053.3818359375...7053.35888671875...7053.3583984375...7053.35888671875...7053.35791015625...7053.35791015625...7053.35791015625...7053.3583984375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7549411058425903\n"
     ]
    }
   ],
   "source": [
    "# Construct prior distribution.\n",
    "bne_prior, bne_gp_config = bne_model_dist(\n",
    "    inputs=X_train_mcmc,\n",
    "    mean_preds=means_train_mcmc,\n",
    "    **bne_model_config)\n",
    "\n",
    "bne_model_config.update(bne_gp_config)\n",
    "print(f'prior model graph: {bne_prior.resolve_graph()}')\n",
    "# Estimates GP weight posterior using MCMC.\n",
    "bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                           model_config=bne_gp_config,\n",
    "                                           Y=Y_train_mcmc,\n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config,\n",
    "                                           initialize_from_map=True)\n",
    "# Generates the posterior sample for all model parameters. \n",
    "bne_joint_samples = make_bne_samples(X_test1,\n",
    "                                     mean_preds=means_test_mcmc,\n",
    "                                     bne_model_config=bne_model_config,\n",
    "                                     bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                     seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_pred_bne_vs = {k: np.mean(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "surface_var_bne_vs = {k: np.var(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_DATA_ADDR_PREFIX = \"./example/data\"\n",
    "# Tuning Parameters\n",
    "BMA_lenthscale = bma_gp_lengthscale\n",
    "BNE_lenthscale = bne_gp_lengthscale\n",
    "BMA_L2 = bma_gp_l2_regularizer\n",
    "BNE_L2 = bne_gp_l2_regularizer\n",
    "_SAVE_ADDR_PREFIX = \"./pic/BMA_lenthscale_{}_L2_{}_BNE_lenthscale_{}_L2_{}\".format(BMA_lenthscale, BMA_L2, BNE_lenthscale, BNE_L2)\n",
    "\n",
    "path=_SAVE_ADDR_PREFIX\n",
    "isExists=os.path.exists(path) #判断路径是否存在，存在则返回true\n",
    "\n",
    "if not isExists:\n",
    "    os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The predictive surface of individual base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = np.asarray(base_model_predictions_eastMA[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "monitors = np.asarray(training_eastMA_noMI[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "base_model_names = [\"pred_av\", \"pred_gs\", \"pred_caces\"]\n",
    "\n",
    "base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]] = np.where(np.isnan(base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]]), 0, base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]])\n",
    "color_norm_base = make_color_norm(\n",
    "    base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]],   \n",
    "    method=\"percentile\")\n",
    "\n",
    "for base_model_name in base_model_names:\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                             'base_model_{}_bmals_{}_r_{}_bnels_{}_r_{}.png'.format(\n",
    "                                 base_model_name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                 bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    \n",
    "    posterior_heatmap_2d(base_model_predictions_eastMA[base_model_name], coordinate,\n",
    "                         monitors,\n",
    "                         cmap='RdYlGn_r',\n",
    "                         norm=color_norm_base, \n",
    "                         #norm_method=\"percentile\",\n",
    "                         save_addr=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The predictive surface of individual BNE gp weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bma_ensemble_weights = bma_joint_samples['ensemble_weights']\n",
    "ensemble_weights_val = tf.reduce_mean(bma_ensemble_weights, axis=0)\n",
    "\n",
    "weights_dict = {\n",
    "    \"AV\": ensemble_weights_val[:, 0],\n",
    "    \"GS\": ensemble_weights_val[:,1],\n",
    "    \"CACES\": ensemble_weights_val[:,2],\n",
    "}\n",
    "#weights_dict\n",
    "color_norm_weights = make_color_norm(\n",
    "    list(weights_dict.values()),#[2],   \n",
    "    method=\"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_weights_var = np.var(bma_ensemble_weights, axis=0)\n",
    "weights_var_dict = {\n",
    "    \"AV\": ensemble_weights_var[:, 0],\n",
    "    \"GS\": ensemble_weights_var[:,1],\n",
    "    \"CACES\": ensemble_weights_var[:,2],\n",
    "}\n",
    "#weights_dict\n",
    "color_norm_weights_var = make_color_norm(\n",
    "    list(weights_var_dict.values()),#[0],   \n",
    "    method=\"percentile\")\n",
    "# display(ensemble_weights_val,ensemble_weights_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_names = [\"AV\", \"GS\", \"CACES\"]\n",
    "for base_model_name in base_model_names:\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                             'base_weights_{}_bmals_{}_r_{}_bnels_{}_r_{}.png'.format(\n",
    "                                 base_model_name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                 bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    \n",
    "    posterior_heatmap_2d(weights_dict[base_model_name], coordinate,\n",
    "                         monitors,\n",
    "                         cmap='viridis',\n",
    "                         norm=color_norm_weights, \n",
    "                         #norm_method=\"percentile\",\n",
    "                         #save_addr='')\n",
    "                         save_addr=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights' variance\n",
    "for base_model_name in base_model_names:\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                             'base_wvar_{}_bmals_{}_r_{}_bnels_{}_r_{}.png'.format(\n",
    "                                 base_model_name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                 bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    \n",
    "    posterior_heatmap_2d(weights_var_dict[base_model_name], coordinate,\n",
    "                         monitors,\n",
    "                         cmap='viridis',\n",
    "                         norm=color_norm_weights_var, \n",
    "                         #norm_method=\"percentile\",\n",
    "                         #save_addr='')\n",
    "                         save_addr=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The predictive surface of Y_mean, residual process, and Y_mean + residual process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BAE\n",
    "color_norm_pred = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[:2][0]),\n",
    "    list(surface_pred_bae.values())[:2],  \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_pred_r = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[2:]),\n",
    "    list(surface_pred_bae.values())[2],  \n",
    "    method=\"residual_percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_pred_bae.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'BAE_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "\n",
    "    value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                                  cmap='RdYlGn_r',\n",
    "                    norm= color_norm_pred_r if name=='resid' else color_norm_pred,\n",
    "                    #norm_method=\"percentile\",\n",
    "                    #save_addr='')\n",
    "                    save_addr=save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE vo\n",
    "color_norm_pred = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[:2][0]),\n",
    "    list(surface_pred_bne_vo.values())[:2],  \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_pred_r = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[2:]),\n",
    "    list(surface_pred_bne_vo.values())[2],  \n",
    "    method=\"residual_percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_pred_bne_vo.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'BNEvo_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "\n",
    "    value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                                  cmap='RdYlGn_r',\n",
    "                    norm= color_norm_pred_r if name=='resid' else color_norm_pred,\n",
    "                                      save_addr=save_name)\n",
    "                    #norm_method=\"percentile\",\n",
    "                    #save_addr='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE v+s\n",
    "color_norm_pred = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[:2][0]),\n",
    "    list(surface_pred_bne_vs.values())[:2],  \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_pred_r = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[2:]),\n",
    "    list(surface_pred_bne_vs.values())[2],  \n",
    "    method=\"residual_percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_pred_bne_vs.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'BNEvs_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(name, bma_gp_lengthscale, \n",
    "                                bma_gp_l2_regularizer, bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "\n",
    "    value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                                  cmap='RdYlGn_r',\n",
    "                    norm= color_norm_pred_r if name=='resid' else color_norm_pred,\n",
    "                    #norm_method=\"percentile\",\n",
    "                    #save_addr='')\n",
    "                save_addr=save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.The predictive variance of Y_mean, residual process, and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAE\n",
    "color_norm_var = make_color_norm(\n",
    "    list(surface_var_bae.values())[:2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_var_r = make_color_norm(\n",
    "    list(surface_var_bae.values())[2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_var_bae.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'var_BAE_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    #value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                cmap='inferno_r',\n",
    "                                norm= color_norm_var_r if name=='resid' else color_norm_var,\n",
    "                                #norm_method=\"percentile\",\n",
    "                                save_addr=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE vo\n",
    "color_norm_var = make_color_norm(\n",
    "    list(surface_var_bne_vo.values())[:2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_var_r = make_color_norm(\n",
    "    list(surface_var_bne_vo.values())[2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_var_bne_vo.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'var_BNEvo_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    #value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                cmap='inferno_r',\n",
    "                                norm= color_norm_var_r if name=='resid' else color_norm_var,\n",
    "                                #norm_method=\"percentile\",\n",
    "                                save_addr=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE v+s\n",
    "color_norm_var = make_color_norm(\n",
    "    list(surface_var_bne_vs.values())[:2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_var_r = make_color_norm(\n",
    "    list(surface_var_bne_vs.values())[2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_var_bne_vs.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'var_BNEvs_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    #value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                cmap='inferno_r',\n",
    "                                norm= color_norm_var_r if name=='resid' else color_norm_var,\n",
    "                                #norm_method=\"percentile\",\n",
    "                                save_addr=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWSzplhQKdqt"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Simulation: compute_metrics\n",
    "def compute_metrics(data_dict, q_true=None, ind_ids=None, num_sample=None):\n",
    "  if q_true is None:\n",
    "    q_true = np.array(\n",
    "        [0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25,\n",
    "         0.75, 0.8, 0.85, 0.9, 0.925, 0.95, 0.975])\n",
    "\n",
    "  if ind_ids is None:\n",
    "    # Find IDs of in-domain test data via range comparison \n",
    "    # between X_train and X_test.\n",
    "    X_train_min = np.min(data_dict['X_train'], axis=0)\n",
    "    X_train_max = np.max(data_dict['X_train'], axis=0)\n",
    "\n",
    "    test_ids_greater_than_min = np.all(\n",
    "        data_dict['X_test'] > X_train_min, axis=-1)\n",
    "    test_ids_less_than_max = np.all(\n",
    "        data_dict['X_test'] < X_train_max, axis=-1)\n",
    "\n",
    "    ind_ids = np.where(\n",
    "        np.logical_and(test_ids_greater_than_min, test_ids_less_than_max))[0]\n",
    "\n",
    "  samples = data_dict[f'{model_name}_samples']\n",
    "  means_true = data_dict['mean_test']\n",
    "  y_test = data_dict['Y_test']\n",
    "\n",
    "  if num_sample is not None:\n",
    "    samples = samples[:num_sample]\n",
    "\n",
    "  means_pred = np.mean(samples, axis=0)\n",
    "  stds_pred = np.std(samples, axis=0)\n",
    "  quantile_pred = np.quantile(samples, q=q_true, axis=0)\n",
    "\n",
    "  # Compute in-domain metrics.\n",
    "  nll_ind = np.mean(\n",
    "      ((means_pred[ind_ids] - means_true[ind_ids])/stds_pred[ind_ids])**2 + \n",
    "      np.log(stds_pred[ind_ids]))\n",
    "  clb_ind = np.mean(\n",
    "      ((means_pred[ind_ids] - means_true[ind_ids])/stds_pred[ind_ids])**2)\n",
    "  shp_ind = np.mean(np.log(stds_pred[ind_ids]))\n",
    "  mse_ind = np.mean(\n",
    "      (means_pred[ind_ids] - means_true[ind_ids])**2) / np.var(means_true[ind_ids])\n",
    "\n",
    "  q_pred_ind = np.mean(y_test[ind_ids] < quantile_pred[:, ind_ids], axis=(1, 2))\n",
    "  ece_ind = np.mean((q_pred_ind - q_true)**2)\n",
    "  cov_prob_95_ind = q_pred_ind[-1] - q_pred_ind[0]\n",
    "  cov_prob_90_ind = q_pred_ind[-2] - q_pred_ind[1]\n",
    "  cov_prob_85_ind = q_pred_ind[-3] - q_pred_ind[2]\n",
    "  cov_prob_80_ind = q_pred_ind[-4] - q_pred_ind[3]\n",
    "\n",
    "  # Compute all-domain (ind + ood) metrics.\n",
    "  nll_all = np.mean(((means_pred - means_true)/stds_pred)**2 + np.log(stds_pred))\n",
    "  clb_all = np.mean(((means_pred - means_true)/stds_pred)**2)\n",
    "  shp_all = np.mean(np.log(stds_pred))\n",
    "  mse_all = np.mean((means_pred - means_true)**2) / np.var(means_true)\n",
    "\n",
    "  q_pred_all = np.mean(y_test < quantile_pred, axis=(1, 2))\n",
    "  ece_all = np.mean((q_pred_all - q_true)**2)\n",
    "  cov_prob_95_all = q_pred_all[-1] - q_pred_all[0]\n",
    "  cov_prob_90_all = q_pred_all[-2] - q_pred_all[1]\n",
    "  cov_prob_85_all = q_pred_all[-3] - q_pred_all[2]\n",
    "  cov_prob_80_all = q_pred_all[-4] - q_pred_all[3]\n",
    "\n",
    "  return (mse_ind, nll_ind, clb_ind, shp_ind, ece_ind, cov_prob_95_ind, cov_prob_90_ind, cov_prob_85_ind, cov_prob_80_ind,\n",
    "          mse_all, nll_all, clb_all, shp_all, ece_all, cov_prob_95_all, cov_prob_90_all, cov_prob_85_all, cov_prob_80_all)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FyVOAW4EODnT",
    "ebzyBOEoNQ_a",
    "vAgjEq1-dty-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('BNE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcc6cf470fa43d25f7728c2f3c746fc9b5580c34ff527761ed8536047e15184c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
