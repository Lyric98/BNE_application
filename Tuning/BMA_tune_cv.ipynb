{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 17:20:45.235472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/gpflow/experimental/utils.py:43: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  f\"You're calling {name} which is considered *experimental*.\"\n",
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/gpflow/experimental/utils.py:43: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  f\"You're calling {name} which is considered *experimental*.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0. Expected: 2.7.0\n",
      "TensorFlow Probability version: 0.18.0. Expected: 0.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 17:21:02.565403: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from wrapper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAYcMR698j6J"
   },
   "source": [
    "# Experiment II: 2D Spatial Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHthi3uPKLkr"
   },
   "source": [
    "### Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KDl4Di8lKIgm"
   },
   "outputs": [],
   "source": [
    "# Optimization configs. \n",
    "# Consider reduce below parameters / set to `False` if MCMC is taking too long:\n",
    "# mcmc_num_steps, mcmc_burnin, mcmc_nchain, mcmc_initialize_from_map.\n",
    "map_step_size=5e-4   # @param\n",
    "map_num_steps=10_000  # @param\n",
    "\n",
    "mcmc_step_size=1e-4 # @param\n",
    "mcmc_num_steps=1000 # @param\n",
    "\n",
    "mcmc_nchain=10 # @param\n",
    "mcmc_burnin=2_500 # @param\n",
    "bne_mcmc_initialize_from_map=\"True\" # @param [\"False\", \"True\"]\n",
    "\n",
    "bne_mcmc_initialize_from_map = eval(bne_mcmc_initialize_from_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AMIJk1OdKIj7"
   },
   "outputs": [],
   "source": [
    "# BMA parameters.\n",
    "y_noise_std = 0.01  # Note: Changed from 0.1 # @param\n",
    "bma_gp_lengthscale = .05 # @param\n",
    "bma_gp_l2_regularizer = 0.1 # @param\n",
    "\n",
    "bma_n_samples_train = 100 # @param\n",
    "bma_n_samples_eval = 250 # @param\n",
    "bma_n_samples_test = 250 # @param\n",
    "bma_seed = 0 # @param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9iyxQiCJKIn7"
   },
   "outputs": [],
   "source": [
    "# BNE parameters.\n",
    "bne_gp_lengthscale = .05 # 5. # @param\n",
    "bne_gp_l2_regularizer = 1. # 15 # @param\n",
    "bne_variance_prior_mean = -2.5 # @param\n",
    "bne_skewness_prior_mean = -2.5 # @param\n",
    "bne_seed = 0 # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training/prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(55, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(84421, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred longitude max and min -69.93 -73.5\n",
      "pred latitude max and min 44.3 40.6\n",
      "train longitude max and min -70.023598 -73.443056\n",
      "train latitude max and min 44.107524 40.74529\n"
     ]
    }
   ],
   "source": [
    "training_eastMA = pd.read_csv('./data/training_dataset/training_eastMA.csv')\n",
    "training_eastMA_noMI = training_eastMA[:51]\n",
    "training_eastMA_folds = pd.read_csv('./data/training_dataset/training_eastMA_folds.csv')\n",
    "base_model_predictions_eastMA = pd.read_csv('./data/prediction_dataset/base_model_predictions_eastMA.csv')\n",
    "display(training_eastMA.shape, training_eastMA_folds.shape, base_model_predictions_eastMA.shape)\n",
    "print(\"pred longitude max and min\", base_model_predictions_eastMA[\"lon\"].max(),base_model_predictions_eastMA[\"lon\"].min())\n",
    "print(\"pred latitude max and min\", base_model_predictions_eastMA[\"lat\"].max(),base_model_predictions_eastMA[\"lat\"].min())\n",
    "#list(base_model_predictions_eastMA.columns)\n",
    "print(\"train longitude max and min\", training_eastMA[\"lon\"].max(),training_eastMA[\"lon\"].min())\n",
    "print(\"train latitude max and min\", training_eastMA[\"lat\"].max(),training_eastMA[\"lat\"].min())\n",
    "\n",
    "\n",
    "training51= pd.read_csv('./data/training_dataset/training51.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 center and scale:  [-72.185104  42.680347] [3.5699997 3.7000008]\n"
     ]
    }
   ],
   "source": [
    "# standardize\n",
    "X_train1 = np.asarray(training_eastMA_noMI[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "X_test1 = np.asarray(base_model_predictions_eastMA[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "X_valid = np.concatenate((X_train1, X_test1), axis=0)\n",
    "X_centr = np.mean(X_valid, axis=0)\n",
    "X_scale = np.max(X_valid, axis=0) - np.min(X_valid, axis=0)\n",
    "\n",
    "X_train1 = (X_train1 - X_centr) / X_scale\n",
    "X_test1 = (X_test1 - X_centr) / X_scale\n",
    "\n",
    "Y_train = np.expand_dims(training_eastMA_noMI[\"aqs\"], 1).astype(np.float32)\n",
    "#Y_test = np.expand_dims(base_model_predictions_eastMA[\"pred_av\"], 1).astype(np.float32)\n",
    "\n",
    "print(\"2011 center and scale: \", X_centr, X_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([51, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([84421, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_names = [\"pred_av\", \"pred_gs\", \"pred_caces\"]\n",
    "base_preds_train = tf.stack([training_eastMA_noMI[base_model_name].astype(np.float32) for base_model_name in base_model_names], axis=-1)\n",
    "base_preds_test = tf.stack([base_model_predictions_eastMA[base_model_name].astype(np.float32) for base_model_name in base_model_names], axis=-1)\n",
    "#base_preds_test\n",
    "display(base_preds_train.shape, base_preds_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Model Averaging\n",
    "\n",
    "A Bayesian ensemble model where ensemble weights $w_k's$ are parameterized by Gaussian process priors:\n",
    "\n",
    "$y \\sim N(\\mu(x), \\sigma^2)$ \n",
    "\n",
    "$\\mu(x) = \\sum_{k=1}^K w_k(x) * m_k(x) \\quad$  where $\\{m_k\\}_{k=1}^K$ are base model predictions.\n",
    "\n",
    "$w(x) = softmax(f(x)) \\qquad\\;\\;\\;$ where $w=[w_1, \\dots, w_K]$ and $f=[f_1, \\dots, f_K]$\n",
    "\n",
    "$f \\stackrel{i.i.d.}{\\sim} GaussianProcess(0, k)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble into configs.\n",
    "bma_model_config = DEFAULT_GP_CONFIG.copy()\n",
    "map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "bma_model_config.update(dict(lengthscale=bma_gp_lengthscale,\n",
    "                             l2_regularizer=bma_gp_l2_regularizer,\n",
    "                             y_noise_std=y_noise_std))\n",
    "\n",
    "map_config.update(dict(learning_rate=map_step_size,\n",
    "                       num_steps=map_num_steps))\n",
    "\n",
    "mcmc_config.update(dict(step_size=mcmc_step_size, \n",
    "                        num_steps=mcmc_num_steps,\n",
    "                       burnin=mcmc_burnin,\n",
    "                       nchain=mcmc_nchain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 10 fold Cross Validation RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_id=10\n",
    "# X_te = X_train1[training51.index[training51[\"fold\"] == fold_id]]\n",
    "# X_tr = X_train1[training51.index[training51[\"fold\"] != fold_id]]\n",
    "# Y_te = Y_train[training51.index[training51[\"fold\"] == fold_id]]\n",
    "# Y_tr = Y_train[training51.index[training51[\"fold\"] != fold_id]]\n",
    "\n",
    "# base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "# base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "# # build model & run MCMC\n",
    "# bma_prior, bma_gp_config = bma_dist(X_tr, \n",
    "#                                     base_preds_tr, \n",
    "#                                     **bma_model_config)\n",
    "\n",
    "# bma_model_config.update(bma_gp_config)\n",
    "\n",
    "\n",
    "# bma_gp_w_samples = run_posterior_inference(model_dist=bma_prior, \n",
    "#                                            model_config=bma_model_config,\n",
    "#                                            Y=Y_tr, \n",
    "#                                            map_config=map_config,\n",
    "#                                            mcmc_config=mcmc_config)\n",
    "\n",
    "\n",
    "# bma_joint_samples = make_bma_samples(X_te, None, base_preds_te, \n",
    "#                                      bma_weight_samples=bma_gp_w_samples[0],\n",
    "#                                      bma_model_config=bma_model_config,\n",
    "#                                      n_samples=bma_n_samples_eval, \n",
    "#                                      seed=bne_seed,\n",
    "#                                      y_samples_only=False)\n",
    "# y_pred = bma_joint_samples['y']\n",
    "# y_pred = tf.reduce_mean(y_pred, axis=0)\n",
    "\n",
    "# print(rmse(Y_te, y_pred))\n",
    "\n",
    "# # Construct data from BMA samples, shapes (num_samples * num_data, ...)\n",
    "# means_tr_mcmc, X_tr_mcmc, Y_tr_mcmc = make_bma_samples(\n",
    "#     X_tr, Y_tr, base_preds_tr, \n",
    "#     bma_weight_samples=bma_gp_w_samples[0],\n",
    "#     bma_model_config=bma_model_config,\n",
    "#     n_samples=bma_n_samples_train,\n",
    "#     seed=bma_seed, \n",
    "#     prepare_mcmc_training=True)\n",
    "\n",
    "# # Mean samples based on test data, shape (num_samples, num_data, num_output).\n",
    "# # It is used to generate final examples in `make_bne_samples()`.\n",
    "# means_te_mcmc = make_bma_samples(\n",
    "#     X_te, None, base_preds_te, \n",
    "#     bma_weight_samples=bma_gp_w_samples[0],\n",
    "#     bma_model_config=bma_model_config,\n",
    "#     n_samples=bma_n_samples_test,\n",
    "#     seed=bma_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer OrthogonalRandomFeatures is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMA model config:  {'lengthscale': 0.05, 'l2_regularizer': 0.1, 'hidden_units': 1024, 'y_noise_std': 0.01, 'units': 3, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t296986.125...114104.2421875...105060.953125...101405.3984375...99391.765625...98193.171875...97492.0234375...97085.65625...96843.9375...96694.2109375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6797609925270081\n",
      "[1.7436911]\n",
      "BNE model config:  {'estimate_mean': True, 'estimate_variance': False, 'estimate_skewness': False, 'variance_prior_mean': 0.0, 'skewness_prior_mean': 0.0, 'lengthscale': 0.05, 'l2_regularizer': 1.0, 'hidden_units': 1024, 'y_noise_std': -1.0, 'units': 2, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t8963.25390625...6418.03564453125...6255.54052734375...6202.41796875...6183.62060546875...6175.2978515625...6171.0751953125...6169.01806640625...6168.1181640625...6167.7998046875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7603747248649597\n",
      "[3.321176]\n",
      "BMA model config:  {'lengthscale': 0.05, 'l2_regularizer': 0.1, 'hidden_units': 1024, 'y_noise_std': 0.01, 'units': 3, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t407858.78125...142104.09375...128451.046875...123064.8125...119508.3671875...117996.09375...117156.328125...116582.1640625...116114.5625...115722.515625...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7246684432029724\n",
      "[1.7436911, 0.49500692]\n",
      "BNE model config:  {'estimate_mean': True, 'estimate_variance': False, 'estimate_skewness': False, 'variance_prior_mean': 0.0, 'skewness_prior_mean': 0.0, 'lengthscale': 0.05, 'l2_regularizer': 1.0, 'hidden_units': 1024, 'y_noise_std': -1.0, 'units': 2, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t11397.314453125...8361.2294921875...8108.11865234375...7994.8515625...7924.82177734375...7879.6689453125...7850.8193359375...7833.5...7823.818359375...7818.60546875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.8151731491088867\n",
      "[3.321176, 3.5356996]\n",
      "BMA model config:  {'lengthscale': 0.05, 'l2_regularizer': 0.1, 'hidden_units': 1024, 'y_noise_std': 0.01, 'units': 3, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t397404.0625...143293.953125...130908.375...125844.78125...118506.7734375...116487.5390625...115436.1875...114396.3984375...112657.578125...112250.28125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.6463499069213867\n",
      "[1.7436911, 0.49500692, 0.5771352]\n",
      "BNE model config:  {'estimate_mean': True, 'estimate_variance': False, 'estimate_skewness': False, 'variance_prior_mean': 0.0, 'skewness_prior_mean': 0.0, 'lengthscale': 0.05, 'l2_regularizer': 1.0, 'hidden_units': 1024, 'y_noise_std': -1.0, 'units': 2, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t12768.6865234375...8976.623046875...8601.1630859375...8416.345703125...8324.173828125...8272.931640625...8237.87109375...8212.19921875...8193.357421875...8179.57373046875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.8575616478919983\n",
      "[3.321176, 3.5356996, 3.830437]\n",
      "BMA model config:  {'lengthscale': 0.05, 'l2_regularizer': 0.1, 'hidden_units': 1024, 'y_noise_std': 0.01, 'units': 3, 'seed': 0} mcmc_config:  {'step_size': 0.0001, 'num_steps': 1000, 'burnin': 2500, 'nchain': 10, 'seed': 0} map_config:  {'learning_rate': 0.0005, 'num_steps': 10000}\n",
      "Running MAP:\t369996.3125...129004.421875...117925.96875...114418.265625...112788.6484375..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/19/s4zdyd_16mb8h239mgwds83m0000gn/T/ipykernel_1649/1041133485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                            \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                            \u001b[0mmap_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                            mcmc_config=mcmc_config)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BNE_application/wrapper_functions.py\u001b[0m in \u001b[0;36mrun_posterior_inference\u001b[0;34m(model_dist, Y, mcmc_config, map_config, model_config, initialize_from_map)\u001b[0m\n\u001b[1;32m    266\u001b[0m     init_state = run_map(target_log_prob_fn=target_log_prob_fn, \n\u001b[1;32m    267\u001b[0m                          \u001b[0mgp_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                          **map_config)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmcmc_nchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/BNE_application/wrapper_functions.py\u001b[0m in \u001b[0;36mrun_map\u001b[0;34m(target_log_prob_fn, gp_config, learning_rate, num_steps, print_every, seed)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{loss().numpy()}...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m             )\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribution_strategy_context.get_replica_context().merge_call(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, apply_state, name)\u001b[0m\n\u001b[1;32m    823\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    885\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwill\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m     \"\"\"\n\u001b[0;32m--> 887\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m       assign_add_op = gen_resource_variable_ops.assign_add_variable_op(\n\u001b[1;32m    889\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initializer_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;34m\"\"\"The handle by which this variable can be accessed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_bma = []\n",
    "rmse_bne = []\n",
    "\n",
    "for fold_id in range(1, 11):\n",
    "    # prepare cross-validation data\n",
    "    X_te = X_train1[training51.index[training51[\"fold\"] == fold_id]]\n",
    "    X_tr = X_train1[training51.index[training51[\"fold\"] != fold_id]]\n",
    "    Y_te = Y_train[training51.index[training51[\"fold\"] == fold_id]]\n",
    "    Y_tr = Y_train[training51.index[training51[\"fold\"] != fold_id]]\n",
    "\n",
    "    base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "    base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "    # build model & run MCMC\n",
    "    bma_prior, bma_gp_config = bma_dist(X_tr, \n",
    "                                    base_preds_tr, \n",
    "                                    **bma_model_config)\n",
    "\n",
    "    bma_model_config.update(bma_gp_config)\n",
    "\n",
    "    print(\"BMA model config: \", bma_model_config, \"mcmc_config: \", mcmc_config, \"map_config: \", map_config)\n",
    "    bma_gp_w_samples = run_posterior_inference(model_dist=bma_prior, \n",
    "                                           model_config=bma_model_config,\n",
    "                                           Y=Y_tr, \n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config)\n",
    "\n",
    "\n",
    "    bma_joint_samples = make_bma_samples(X_te, None, base_preds_te, \n",
    "                                     bma_weight_samples=bma_gp_w_samples[0],\n",
    "                                     bma_model_config=bma_model_config,\n",
    "                                     n_samples=bma_n_samples_eval, \n",
    "                                     seed=bne_seed,\n",
    "                                     y_samples_only=False)\n",
    "\n",
    "    y_pred = bma_joint_samples['y']\n",
    "    y_pred = tf.reduce_mean(y_pred, axis=0)\n",
    "    rmse_bma.append(rmse(Y_te, y_pred))\n",
    "    print(rmse_bma)\n",
    "\n",
    "    # Construct data from BMA samples, shapes (num_samples * num_data, ...)\n",
    "    means_tr_mcmc, X_tr_mcmc, Y_tr_mcmc = make_bma_samples(\n",
    "        X_tr, Y_tr, base_preds_tr, \n",
    "        bma_weight_samples=bma_gp_w_samples[0],\n",
    "        bma_model_config=bma_model_config,\n",
    "        n_samples=bma_n_samples_train,\n",
    "        seed=bma_seed, \n",
    "        prepare_mcmc_training=True)\n",
    "\n",
    "    # Mean samples based on test data, shape (num_samples, num_data, num_output).\n",
    "    # It is used to generate final examples in `make_bne_samples()`.\n",
    "    means_te_mcmc = make_bma_samples(\n",
    "        X_te, None, base_preds_te, \n",
    "        bma_weight_samples=bma_gp_w_samples[0],\n",
    "        bma_model_config=bma_model_config,\n",
    "        n_samples=bma_n_samples_test,\n",
    "        seed=bma_seed)\n",
    "\n",
    "    # # # BNE GP Configs.\n",
    "# # lengthscale = 1. # @param\n",
    "# # l2_regularizer = 10. # @param\n",
    "\n",
    "# BNE model configs. \n",
    "# If estimate_mean=False, only estimates a constant variance on top of the \n",
    "# original model.\n",
    "    estimate_mean = \"True\" # @param [\"True\", \"False\"]\n",
    "    variance_prior_mean=0. # @param\n",
    "# # MAP and MCMC configs\n",
    "# map_step_size=0.1 # @param\n",
    "# map_num_steps=10_000 # @param\n",
    "\n",
    "# mcmc_step_size=1e-2 # @param\n",
    "# mcmc_num_steps=10_000 # @param\n",
    "\n",
    "    bne_gp_config = DEFAULT_GP_CONFIG.copy()\n",
    "    bne_model_config = DEFAULT_BNE_CONFIG.copy()\n",
    "\n",
    "    map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "    mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "\n",
    "    bne_gp_config.update(dict(lengthscale=bne_gp_lengthscale, \n",
    "                          l2_regularizer=bne_gp_l2_regularizer))\n",
    "    bne_model_config.update(dict(estimate_mean=eval(estimate_mean),\n",
    "                             variance_prior_mean=variance_prior_mean,\n",
    "                             **bne_gp_config))\n",
    "\n",
    "    map_config.update(dict(learning_rate=map_step_size,\n",
    "                       num_steps=map_num_steps))\n",
    "    mcmc_config.update(dict(step_size=mcmc_step_size, \n",
    "                        num_steps=mcmc_num_steps,\n",
    "                       burnin=mcmc_burnin,\n",
    "                       nchain=mcmc_nchain))\n",
    "    \n",
    "    # build model & run MCMC\n",
    "    bne_prior, bne_gp_config = bne_model_dist(\n",
    "            inputs=X_tr_mcmc,\n",
    "            mean_preds=means_tr_mcmc,\n",
    "            **bne_model_config)\n",
    "\n",
    "    bne_model_config.update(bne_gp_config)\n",
    "\n",
    "    print(\"BNE model config: \", bne_model_config, \"mcmc_config: \", mcmc_config, \"map_config: \", map_config)\n",
    "    # Estimates GP weight posterior using MCMC.\n",
    "    bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                            model_config=bne_gp_config,\n",
    "                                            Y=Y_tr_mcmc,\n",
    "                                            map_config=map_config,\n",
    "                                            mcmc_config=mcmc_config,\n",
    "                                            initialize_from_map=True)\n",
    "    # Generates the posterior sample for all model parameters. \n",
    "    bne_joint_samples = make_bne_samples(X_te,\n",
    "                                        mean_preds=means_te_mcmc,\n",
    "                                        bne_model_config=bne_model_config,\n",
    "                                        bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                        seed=bne_seed)\n",
    "    \n",
    "    y_pred = bne_joint_samples['y']\n",
    "    means_pred = np.mean(y_pred, axis=0)\n",
    "    rmse_bne.append(rmse(Y_te, y_pred))\n",
    "    print(rmse_bne)\n",
    "\n",
    "    \n",
    "print(rmse_bma, rmse_bne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.97563 ]\n",
      " [ 9.485   ]\n",
      " [ 9.969748]\n",
      " [10.035838]\n",
      " [10.041177]\n",
      " [ 9.837391]]\n"
     ]
    }
   ],
   "source": [
    "means_pred - Y_train[training51.index[training51[\"fold\"] == 2]]\n",
    "print(Y_train[training51.index[training51[\"fold\"] == 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('gp_weights', ()), ('y', ('gp_weights',)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bma_prior, bma_gp_config = bma_dist(X_train1, \n",
    "                                    base_preds_train, \n",
    "                                    **bma_model_config)\n",
    "\n",
    "bma_model_config.update(bma_gp_config)\n",
    "\n",
    "# Check if the model graph is specified correctly.\n",
    "bma_prior.resolve_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t408849.5625...142495.578125...131517.390625...125986.09375...122971.6171875...121366.5078125...120428.7734375...119888.234375...119569.5859375...119269.6015625...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.704208493232727\n"
     ]
    }
   ],
   "source": [
    "# bma_gp_w_samples, chain_samples, sampler_stat = run_posterior_inference(model_dist=bma_prior, \n",
    "#                                            model_config=bma_model_config,\n",
    "#                                            Y=Y_train, \n",
    "#                                            map_config=map_config,\n",
    "#                                            mcmc_config=mcmc_config)\n",
    "\n",
    "# Above the debug mode\n",
    "\n",
    "bma_gp_w_samples = run_posterior_inference(model_dist=bma_prior, \n",
    "                                           model_config=bma_model_config,\n",
    "                                           Y=Y_train, \n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config)\n",
    "\n",
    "\n",
    "bma_joint_samples = make_bma_samples(X_test1, None, base_preds_test, \n",
    "                                     bma_weight_samples=bma_gp_w_samples[0],\n",
    "                                     bma_model_config=bma_model_config,\n",
    "                                     n_samples=bma_n_samples_eval, \n",
    "                                     seed=bne_seed,\n",
    "                                     y_samples_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check with Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE:  1.6748887\n"
     ]
    }
   ],
   "source": [
    "#np.mean((means_pred - means_true)**2) / np.var(means_true)\n",
    "# mse = tf.reduce_mean((means_train_mcmc-Y_train_mcmc)** 2)/ np.var(Y_train_mcmc)\n",
    "# mse\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# def rmse(y_obs, y_pred):\n",
    "#     return np.sqrt(np.mean((y_obs - y_pred) ** 2))\n",
    "\n",
    "\n",
    "reg = LinearRegression().fit(X_train1, Y_train)\n",
    "y_pred = reg.predict(X_train1)\n",
    "rmse_lr = mean_squared_error(y_true=Y_train, y_pred=y_pred, squared=False)\n",
    "print(\"Linear Regression RMSE: \", rmse_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute sample stats.\n",
    "# sample_mean = tf.reduce_mean(means_train_mcmc, axis=[0, 1])\n",
    "# # ==> approx equal to weights\n",
    "\n",
    "# sample_var = tf.reduce_mean(\n",
    "#     tf.math.squared_difference(chain_samples, sample_mean),\n",
    "#     axis=[0, 1])\n",
    "# ==> less than 1\n",
    "\n",
    "# def rmse(y_obs, y_pred):\n",
    "#     \"\"\"Computes root mean square error.\"\"\"\n",
    "#     return np.sqrt(np.mean((y_obs.squeeze() - y_pred.squeeze()) ** 2))\n",
    "# rmse(Y_train_mcmc, means_train_mcmc)\n",
    "\n",
    "# samples = samples[:num_sample]\n",
    "# means_pred = np.mean(samples, axis=0)\n",
    "# stds_pred = np.std(samples, axis=0)\n",
    "# mse_ind = np.mean(\n",
    "#     (means_pred[ind_ids] - means_true[ind_ids])**2) / np.var(means_true[ind_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for BAE/BNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer OrthogonalRandomFeatures is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    }
   ],
   "source": [
    "# Construct data from BMA samples, shapes (num_samples * num_data, ...)\n",
    "means_train_mcmc, X_train_mcmc, Y_train_mcmc = make_bma_samples(\n",
    "    X_train1, Y_train, base_preds_train, \n",
    "    bma_weight_samples=bma_gp_w_samples[0],\n",
    "    bma_model_config=bma_model_config,\n",
    "    n_samples=bma_n_samples_train,\n",
    "    seed=bma_seed, \n",
    "    prepare_mcmc_training=True)\n",
    "\n",
    "# Mean samples based on test data, shape (num_samples, num_data, num_output).\n",
    "# It is used to generate final examples in `make_bne_samples()`.\n",
    "means_test_mcmc = make_bma_samples(\n",
    "    X_test1, None, base_preds_test, \n",
    "    bma_weight_samples=bma_gp_w_samples[0],\n",
    "    bma_model_config=bma_model_config,\n",
    "    n_samples=bma_n_samples_test,\n",
    "    seed=bma_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spcv_df = tf.repeat(training51[\"fold\"], bma_n_samples_train, axis=0)\n",
    "\n",
    "# fold_id=2\n",
    "\n",
    "# X_te = X_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "# X_tr = X_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "# Y_te = Y_train_mcmc[spcv_df==fold_id]\n",
    "# Y_tr = Y_train_mcmc[spcv_df!=fold_id]\n",
    "# means_tr_mcmc = means_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "# means_te_mcmc = means_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "\n",
    "# base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "# base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "# # Construct posterior sampler.\n",
    "# # Construct posterior sampler.\n",
    "# bne_prior, bne_gp_config = bne_model_dist(\n",
    "#     inputs=X_tr,\n",
    "#     mean_preds=means_tr_mcmc,\n",
    "#     **bne_model_config)\n",
    "\n",
    "# bne_model_config.update(bne_gp_config)\n",
    "\n",
    "\n",
    "# # Estimates GP weight posterior using MCMC.\n",
    "# bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "#                                            model_config=bne_gp_config,\n",
    "#                                            Y=Y_tr,\n",
    "#                                            map_config=map_config,\n",
    "#                                            mcmc_config=mcmc_config,\n",
    "#                                            initialize_from_map=True)\n",
    "# # Generates the posterior sample for all model parameters. \n",
    "# bne_joint_samples = make_bne_samples(X_te,\n",
    "#                                      mean_preds=means_te_mcmc,\n",
    "#                                      bne_model_config=bne_model_config,\n",
    "#                                      bne_weight_samples=bne_gp_w_samples[0],\n",
    "#                                      seed=bne_seed)\n",
    "# y_pred = bne_joint_samples['y']\n",
    "# means_pred = np.mean(y_pred, axis=0)\n",
    "# rmse_bne = np.sqrt(np.mean((means_pred - Y_te)**2))\n",
    "# rmse_bne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Additive Ensemble\n",
    "\n",
    "Given $\\mu(x)$ the posterior of a Bayesian ensemble model, the Bayesian Additive Ensemble is defined as:    \n",
    "\n",
    "$y \\sim N(\\mu(x) + r(x), \\sigma^2)$\n",
    "\n",
    "$r \\sim GaussianProcess(0, k)$\n",
    "\n",
    "The additive ensemble $r(x)$ services two purposes: \n",
    "\n",
    "1. Mitigates systematic bias in model prediction; \n",
    "2. Quantifies the model's epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # BNE GP Configs.\n",
    "# # lengthscale = 1. # @param\n",
    "# # l2_regularizer = 10. # @param\n",
    "\n",
    "# BNE model configs. \n",
    "# If estimate_mean=False, only estimates a constant variance on top of the \n",
    "# original model.\n",
    "estimate_mean = \"True\" # @param [\"True\", \"False\"]\n",
    "variance_prior_mean=0. # @param\n",
    "# # MAP and MCMC configs\n",
    "# map_step_size=0.1 # @param\n",
    "# map_num_steps=10_000 # @param\n",
    "\n",
    "# mcmc_step_size=1e-2 # @param\n",
    "# mcmc_num_steps=10_000 # @param\n",
    "\n",
    "bne_gp_config = DEFAULT_GP_CONFIG.copy()\n",
    "bne_model_config = DEFAULT_BNE_CONFIG.copy()\n",
    "\n",
    "map_config = DEFAULT_MAP_CONFIG.copy()\n",
    "mcmc_config = DEFAULT_MCMC_CONFIG.copy()\n",
    "\n",
    "\n",
    "bne_gp_config.update(dict(lengthscale=bne_gp_lengthscale, \n",
    "                          l2_regularizer=bne_gp_l2_regularizer))\n",
    "bne_model_config.update(dict(estimate_mean=eval(estimate_mean),\n",
    "                             variance_prior_mean=variance_prior_mean,\n",
    "                             **bne_gp_config))\n",
    "\n",
    "map_config.update(dict(learning_rate=map_step_size,\n",
    "                       num_steps=map_num_steps))\n",
    "mcmc_config.update(dict(step_size=mcmc_step_size, \n",
    "                        num_steps=mcmc_num_steps,\n",
    "                       burnin=mcmc_burnin,\n",
    "                       nchain=mcmc_nchain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t9345.3330078125...6812.9853515625...6520.54736328125...6457.919921875...6431.615234375...6418.30615234375...6410.1630859375...6405.15673828125...6402.2373046875...6400.5810546875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7659201622009277\n",
      "Running MAP:\t11660.6328125...8991.4853515625...8634.99609375...8541.29296875...8500.9951171875...8478.3271484375...8464.1328125...8455.822265625...8451.666015625...8450.060546875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7294092774391174\n",
      "Running MAP:\t12507.0283203125...8742.0400390625...8424.658203125...8292.90234375...8211.7197265625...8147.400390625...8095.224609375...8053.931640625...8022.33349609375...7998.8974609375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7653064727783203\n",
      "Running MAP:\t12239.8583984375...8483.6669921875...8200.3193359375...8101.00830078125...8036.7373046875...7986.86181640625...7948.244140625...7919.75341796875...7899.66845703125...7885.6552734375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.8057963252067566\n",
      "Running MAP:\t10867.28125...8581.3515625...8471.9990234375...8436.0458984375...8415.576171875...8402.5654296875...8394.0859375...8388.6962890625...8385.46484375...8383.5927734375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7128016948699951\n",
      "Running MAP:\t12289.9365234375...8822.8251953125...8348.2294921875...8147.9326171875...8042.99560546875...7983.87744140625...7949.8291015625...7929.5537109375...7917.4912109375...7910.6328125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7133280634880066\n",
      "Running MAP:\t13003.6640625...8823.0810546875...8500.4423828125...8367.748046875...8301.7314453125...8258.8017578125...8229.7080078125...8209.3583984375...8194.6005859375...8183.5205078125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7165954113006592\n",
      "Running MAP:\t12305.6904296875...8978.2890625...8691.51171875...8576.7373046875...8511.4765625...8465.0400390625...8430.9384765625...8406.40234375...8388.99609375...8376.60546875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.8105015158653259\n",
      "Running MAP:\t11370.36328125...8629.958984375...8205.96484375...8008.791015625...7913.9189453125...7867.77978515625...7842.49658203125...7826.546875...7815.9853515625...7808.84326171875...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.8807346224784851\n",
      "Running MAP:\t12467.19921875...8697.984375...8331.7568359375...8140.8583984375...8035.4501953125...7972.5859375...7932.80712890625...7905.74755859375...7887.185546875...7875.06689453125...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.8046125769615173\n",
      "10 fold cross validation rmse: [2.1268609, 1.5241113, 0.93176067, 1.3353401, 2.5536575, 0.9383487, 0.99414504, 2.2056296, 1.913976, 1.5200524]\n",
      "mean rmse: 1.6043882\n"
     ]
    }
   ],
   "source": [
    "spcv_df = tf.repeat(training51[\"fold\"], bma_n_samples_train, axis=0)\n",
    "rmse_bae = []\n",
    "for fold_id in range(1,11):\n",
    "    X_te = X_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "    X_tr = X_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "    Y_te = Y_train_mcmc[spcv_df==fold_id]\n",
    "    Y_tr = Y_train_mcmc[spcv_df!=fold_id]\n",
    "    means_tr_mcmc = means_train_mcmc.numpy()[spcv_df!=fold_id]\n",
    "    means_te_mcmc = means_train_mcmc.numpy()[spcv_df==fold_id]\n",
    "\n",
    "    base_preds_tr=base_preds_train.numpy()[training51.index[training51[\"fold\"] != fold_id]]\n",
    "    base_preds_te=base_preds_train.numpy()[training51.index[training51[\"fold\"] == fold_id]]\n",
    "\n",
    "    # Construct posterior sampler.\n",
    "    bne_prior, bne_gp_config = bne_model_dist(\n",
    "        inputs=X_tr,\n",
    "        mean_preds=means_tr_mcmc,\n",
    "        **bne_model_config)\n",
    "\n",
    "    bne_model_config.update(bne_gp_config)\n",
    "\n",
    "\n",
    "    # Estimates GP weight posterior using MCMC.\n",
    "    bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                            model_config=bne_gp_config,\n",
    "                                            Y=Y_tr,\n",
    "                                            map_config=map_config,\n",
    "                                            mcmc_config=mcmc_config,\n",
    "                                            initialize_from_map=True)\n",
    "    # Generates the posterior sample for all model parameters. \n",
    "    bne_joint_samples = make_bne_samples(X_te,\n",
    "                                        mean_preds=means_te_mcmc,\n",
    "                                        bne_model_config=bne_model_config,\n",
    "                                        bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                        seed=bne_seed)\n",
    "    \n",
    "    y_pred = bne_joint_samples['y']\n",
    "    means_pred = np.mean(y_pred, axis=0)\n",
    "    rmse_bae.append(np.sqrt(np.mean((means_pred - Y_te)**2)))\n",
    "print(\"10 fold cross validation rmse:\", rmse_bae)\n",
    "print(\"mean rmse:\", np.mean(rmse_bae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([250, 84421, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bne_joint_samples['y']\n",
    "means_pred = np.mean(y_pred, axis=0)\n",
    "rmse_bae=np.sqrt(np.mean((means_pred - Y_te)**2))\n",
    "rmse_bae\n",
    "means_test_mcmc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means_te_mcmc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior model graph: (('gp_weights', ()), ('y', ('gp_weights',)))\n"
     ]
    }
   ],
   "source": [
    "# Construct posterior sampler.\n",
    "bne_prior, bne_gp_config = bne_model_dist(\n",
    "    inputs=X_train_mcmc,\n",
    "    mean_preds=means_train_mcmc,\n",
    "    **bne_model_config)\n",
    "\n",
    "bne_model_config.update(bne_gp_config)\n",
    "print(f'prior model graph: {bne_prior.resolve_graph()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MAP:\t8040.71630859375...7257.259765625...7244.951171875...7239.53125...7238.07568359375...7237.91650390625...7237.91259765625...7237.912109375...7237.912109375...7237.912109375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7726417183876038\n"
     ]
    }
   ],
   "source": [
    "# Estimates GP weight posterior using MCMC.\n",
    "bne_gp_w_samples, chain_samples, sampler_stat = run_posterior_inference(model_dist=bne_prior,\n",
    "                                           model_config=bne_gp_config,\n",
    "                                           Y=Y_train_mcmc,\n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config,\n",
    "                                           initialize_from_map=True)\n",
    "# Generates the posterior sample for all model parameters. \n",
    "bne_joint_samples = make_bne_samples(X_test1,\n",
    "                                     mean_preds=means_test_mcmc,\n",
    "                                     bne_model_config=bne_model_config,\n",
    "                                     bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                     seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_pred_bae = {k: np.mean(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "surface_var_bae = {k: np.var(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "\n",
    "# dealing with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005235664112010045"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentile of NAN\n",
    "np.sum(np.isnan(np.mean(bne_joint_samples['mean_original'], axis=0)))/84421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Nonparametric Ensemble (Variance Only)\n",
    "So far, we are only estimating the mean-component of the model, i.e., we are assuming: \n",
    "\n",
    "$y \\sim Gaussian(m(x), \\sigma^2); \\quad m(x) = GP(0, k)$.\n",
    "\n",
    "By doing so, the model is implicitly assuming the distribution of $y$ is always a symmetric Gaussian distribution with constant mean across space and time. As a result, our model can only quantify model uncertainty (due to lack of data) via the GP prior, but cannot flexibly capture the data uncertainty that is inherent to the empirical distribution of y.\n",
    "\n",
    "To resolve this, we extend the ensemble's outcome distribution $y | f$ by also estimating the higher moments of the data distribution (e.g., variance, skewness, etc) using flexible estimators. Specifically, we specify the outcome distribution family to the [maximum-entropy distribution](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) given the known moments, so the predictive distribution is [minimax](https://arxiv.org/pdf/math/0410076.pdf) and still statistically efficient to estimate.\n",
    "\n",
    "For example, when we want to estimate the first two moments (mean and variance) of the distribution, this leads to a Gaussian distribution with spatio-temporally adaptive variance $\\sigma(x)^2$:\n",
    "\n",
    "$$y \\sim Gaussian(m(x), \\sigma(x)^2); \\quad \\mbox{where} \\quad m \\sim GP(0, k_m), \\sigma \\sim GP(0, k_\\sigma)$$\n",
    "\n",
    "and when we want to estimate the first three moments (mean and variance) of the distribution, this leads to a [Exponentially-modifed Gaussian](https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution) (EMG) distribution with spatio-temporally adaptive variance $\\sigma(x)^2$ and skewness $\\lambda(x)$:\n",
    "\n",
    "$$y \\sim EMG(m(x), \\sigma(x)^2, \\lambda(x)); \\quad \\mbox{where} \\quad m \\sim GP(0, k_m), \\sigma \\sim GP(0, k_\\sigma), \\lambda \\sim GP(0, k_\\lambda)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model & Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liyanran/opt/anaconda3/envs/BNE/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer OrthogonalRandomFeatures is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior model graph: (('gp_weights', ()), ('y', ('gp_weights',)))\n",
      "Running MAP:\t12865.1201171875...9376.412109375...9139.1455078125...9043.2568359375...8973.501953125...8915.640625...8868.724609375...8832.21875...8804.4267578125...8783.3193359375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7179597616195679\n"
     ]
    }
   ],
   "source": [
    "# Construct posterior sampler.\n",
    "bne_prior, bne_gp_config = bne_model_dist(\n",
    "    inputs=X_train_mcmc,\n",
    "    mean_preds=means_train_mcmc,\n",
    "    **bne_model_config)\n",
    "\n",
    "bne_model_config.update(bne_gp_config)\n",
    "print(f'prior model graph: {bne_prior.resolve_graph()}')\n",
    "\n",
    "# Estimates GP weight posterior using MCMC.\n",
    "bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                           model_config=bne_gp_config,\n",
    "                                           Y=Y_train_mcmc,\n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config,\n",
    "                                           initialize_from_map=True)\n",
    "# Generates the posterior sample for all model parameters. \n",
    "bne_joint_samples = make_bne_samples(X_test1,\n",
    "                                     mean_preds=means_test_mcmc,\n",
    "                                     bne_model_config=bne_model_config,\n",
    "                                     bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                     seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 84421, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gp_weights': <tf.Tensor: shape=(250, 1024, 2), dtype=float32, numpy=\n",
       " array([[[ 4.3302817 , -0.12580211],\n",
       "         [-4.5724216 , -0.22668421],\n",
       "         [-2.037432  ,  0.81041896],\n",
       "         ...,\n",
       "         [ 1.47382   ,  2.9655313 ],\n",
       "         [ 7.116604  , -1.8567202 ],\n",
       "         [-3.0230832 ,  0.36611778]],\n",
       " \n",
       "        [[-2.1442125 , -0.09913818],\n",
       "         [-1.90095   ,  2.909925  ],\n",
       "         [-3.675616  ,  2.2472198 ],\n",
       "         ...,\n",
       "         [-2.5811794 ,  1.3216788 ],\n",
       "         [-4.036252  , -1.9933115 ],\n",
       "         [ 1.4075226 ,  2.1063633 ]],\n",
       " \n",
       "        [[ 4.6369514 , -0.10709982],\n",
       "         [-3.0650153 , -1.2449614 ],\n",
       "         [-4.398227  ,  0.35887706],\n",
       "         ...,\n",
       "         [ 2.5732176 ,  2.1438735 ],\n",
       "         [ 5.7858176 , -1.4375418 ],\n",
       "         [ 1.1682687 , -1.7226412 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 4.646091  , -0.03416028],\n",
       "         [-3.0958154 ,  1.5564643 ],\n",
       "         [-2.1034846 ,  1.5209038 ],\n",
       "         ...,\n",
       "         [ 0.6438954 ,  1.198103  ],\n",
       "         [ 7.363459  , -2.5498924 ],\n",
       "         [-3.36166   ,  1.2375103 ]],\n",
       " \n",
       "        [[ 1.6644778 , -0.08639497],\n",
       "         [-0.3062649 , -7.3339696 ],\n",
       "         [ 0.87350905,  1.3603339 ],\n",
       "         ...,\n",
       "         [-3.3344018 , -2.4269228 ],\n",
       "         [ 4.6286716 ,  8.260303  ],\n",
       "         [ 5.63761   , -3.1429513 ]],\n",
       " \n",
       "        [[-2.1442125 , -0.09913818],\n",
       "         [-1.90095   ,  2.909925  ],\n",
       "         [-3.675616  ,  2.2472198 ],\n",
       "         ...,\n",
       "         [-2.5811794 ,  1.3216788 ],\n",
       "         [-4.036252  , -1.9933115 ],\n",
       "         [ 1.4075226 ,  2.1063633 ]]], dtype=float32)>,\n",
       " 'gps': <tf.Tensor: shape=(250, 84421, 2), dtype=float32, numpy=\n",
       " array([[[ 0.04760683,  3.1199803 ],\n",
       "         [-0.10840833,  2.9481525 ],\n",
       "         [-0.23894578,  2.757722  ],\n",
       "         ...,\n",
       "         [-0.43232954, -4.3187523 ],\n",
       "         [-0.2101233 , -4.624567  ],\n",
       "         [ 0.01340938, -4.9459515 ]],\n",
       " \n",
       "        [[-3.6767933 , -1.3488712 ],\n",
       "         [-4.056982  , -1.6051949 ],\n",
       "         [-4.4021955 , -1.8643777 ],\n",
       "         ...,\n",
       "         [ 1.244079  , 10.006768  ],\n",
       "         [ 1.445605  ,  9.898945  ],\n",
       "         [ 1.6487337 ,  9.751624  ]],\n",
       " \n",
       "        [[-0.16933239,  2.2045193 ],\n",
       "         [-0.33962846,  1.9629209 ],\n",
       "         [-0.49001127,  1.7052499 ],\n",
       "         ...,\n",
       "         [ 0.90386665, -3.7866967 ],\n",
       "         [ 1.1941755 , -4.030672  ],\n",
       "         [ 1.4742415 , -4.294857  ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.537873  ,  3.9263487 ],\n",
       "         [-0.5537996 ,  3.7259116 ],\n",
       "         [-0.54471123,  3.5040598 ],\n",
       "         ...,\n",
       "         [ 0.68724954, -4.6560125 ],\n",
       "         [ 0.94733715, -4.9268317 ],\n",
       "         [ 1.2028123 , -5.209466  ]],\n",
       " \n",
       "        [[ 0.36068773,  1.1745338 ],\n",
       "         [ 0.28663036,  0.952548  ],\n",
       "         [ 0.21628958,  0.7091813 ],\n",
       "         ...,\n",
       "         [-3.0887704 ,  4.7319026 ],\n",
       "         [-2.741981  ,  4.9619503 ],\n",
       "         [-2.3768182 ,  5.1602197 ]],\n",
       " \n",
       "        [[-3.6767933 , -1.348872  ],\n",
       "         [-4.0569816 , -1.6051962 ],\n",
       "         [-4.402194  , -1.8643776 ],\n",
       "         ...,\n",
       "         [ 1.2440798 , 10.006769  ],\n",
       "         [ 1.445604  ,  9.898945  ],\n",
       "         [ 1.6487337 ,  9.751624  ]]], dtype=float32)>,\n",
       " 'scale': <tf.Tensor: shape=(250,), dtype=float32, numpy=\n",
       " array([0.8817893 , 0.90561754, 0.89843595, 0.9591377 , 0.9458453 ,\n",
       "        0.9000956 , 0.92833483, 0.92864466, 0.9100601 , 0.86998415,\n",
       "        0.9379965 , 0.9765269 , 0.89916784, 0.973214  , 0.90042734,\n",
       "        0.9241808 , 0.9021235 , 0.97067416, 0.91353625, 0.9746681 ,\n",
       "        0.91034186, 0.9021235 , 0.9353611 , 0.93847775, 0.874605  ,\n",
       "        0.89331406, 0.89369535, 0.914773  , 0.9334254 , 0.9333976 ,\n",
       "        0.9267951 , 0.9387544 , 0.9114958 , 0.9566947 , 0.93830705,\n",
       "        0.92819726, 0.9661012 , 0.9205938 , 0.88849694, 0.8852403 ,\n",
       "        0.88787466, 0.90424174, 0.91448313, 0.8976697 , 0.857389  ,\n",
       "        0.9441899 , 0.89331406, 0.9273219 , 0.84917516, 0.96320814,\n",
       "        0.8851492 , 0.9230444 , 0.93935   , 0.88934   , 0.89095217,\n",
       "        0.8921445 , 0.93830705, 0.86052126, 0.92854756, 0.9661012 ,\n",
       "        0.9567112 , 0.85355866, 0.9530884 , 0.91210544, 0.91453016,\n",
       "        0.91803813, 0.87522787, 0.89666015, 0.9573282 , 0.90089875,\n",
       "        0.9299559 , 0.9387544 , 0.92780906, 0.8938895 , 0.94839096,\n",
       "        0.91210544, 0.9889757 , 0.92871064, 0.88267535, 0.88087887,\n",
       "        0.92325187, 0.938884  , 0.9305737 , 0.9030349 , 0.84979236,\n",
       "        0.9501885 , 0.89843595, 0.92562544, 0.8995937 , 0.94014215,\n",
       "        0.8733099 , 0.95531815, 0.891839  , 0.9081291 , 0.9581874 ,\n",
       "        0.89133275, 0.93671757, 0.898192  , 0.94231516, 0.8526995 ,\n",
       "        0.9450847 , 0.91469437, 0.973214  , 0.9183378 , 0.9081291 ,\n",
       "        0.8811372 , 0.8907051 , 0.87056756, 0.91723186, 0.9127885 ,\n",
       "        0.874605  , 0.9004016 , 0.9367782 , 0.90089875, 0.9127885 ,\n",
       "        0.8864045 , 0.9187338 , 0.91034186, 0.9066766 , 0.94012153,\n",
       "        0.9000956 , 0.90916604, 0.8816074 , 0.92833483, 0.94150335,\n",
       "        0.87024194, 0.93990064, 0.93322617, 0.95531815, 0.93270755,\n",
       "        0.9692948 , 0.9438388 , 0.914773  , 0.8688218 , 0.89670223,\n",
       "        0.93197703, 0.9027271 , 0.90301377, 0.92845404, 0.93677783,\n",
       "        0.87024194, 0.9137151 , 0.91082543, 0.8987716 , 0.9117617 ,\n",
       "        0.97729397, 0.91334337, 0.92191994, 0.9397653 , 0.9166946 ,\n",
       "        0.9368379 , 0.9279571 , 0.89986396, 0.93892473, 0.9267951 ,\n",
       "        0.88116384, 0.91469437, 0.88718516, 0.91227216, 0.94222426,\n",
       "        0.9137151 , 0.8965733 , 0.8976697 , 0.90561754, 0.99622846,\n",
       "        0.9357715 , 0.9149593 , 0.90671676, 0.874605  , 0.8833628 ,\n",
       "        0.9326885 , 0.9397634 , 0.8589508 , 0.93268394, 0.90904176,\n",
       "        0.9042106 , 0.8914675 , 0.90578043, 0.9020967 , 0.9176854 ,\n",
       "        0.92532414, 0.9692948 , 0.9041891 , 0.9522432 , 0.94215983,\n",
       "        0.92948675, 0.92819726, 0.9080161 , 0.964893  , 0.9299559 ,\n",
       "        0.94231516, 0.962035  , 0.91268665, 0.92948675, 0.86561   ,\n",
       "        0.9101425 , 0.9889757 , 0.9175752 , 0.90550447, 0.9615117 ,\n",
       "        0.9542649 , 0.88087887, 0.8921445 , 0.9270614 , 0.9154283 ,\n",
       "        0.8748912 , 0.91693723, 0.9080161 , 0.9175692 , 0.974717  ,\n",
       "        0.9638677 , 0.9776497 , 0.90916604, 0.9344021 , 0.9149593 ,\n",
       "        0.8712671 , 0.94112766, 0.8938895 , 0.9068187 , 0.9187338 ,\n",
       "        0.90550447, 0.91210544, 0.9344021 , 0.92833483, 0.974717  ,\n",
       "        0.93197703, 0.91239065, 0.9528547 , 0.9009533 , 0.8977413 ,\n",
       "        0.9333976 , 0.90869087, 0.914773  , 0.87235516, 0.9573282 ,\n",
       "        0.86052126, 0.89843595, 0.88849694, 0.9692948 , 0.904583  ,\n",
       "        0.95039135, 0.8911551 , 0.93830705, 0.9163564 , 0.9009533 ,\n",
       "        0.9020345 , 0.9164944 , 0.9664166 , 0.91723186, 0.90561754],\n",
       "       dtype=float32)>,\n",
       " 'mean': <tf.Tensor: shape=(250, 84421, 1), dtype=float32, numpy=\n",
       " array([[[ 0.04760683],\n",
       "         [-0.10840833],\n",
       "         [-0.23894578],\n",
       "         ...,\n",
       "         [-0.43232954],\n",
       "         [-0.2101233 ],\n",
       "         [ 0.01340938]],\n",
       " \n",
       "        [[-3.6767933 ],\n",
       "         [-4.056982  ],\n",
       "         [-4.4021955 ],\n",
       "         ...,\n",
       "         [ 1.244079  ],\n",
       "         [ 1.445605  ],\n",
       "         [ 1.6487337 ]],\n",
       " \n",
       "        [[-0.16933239],\n",
       "         [-0.33962846],\n",
       "         [-0.49001127],\n",
       "         ...,\n",
       "         [ 0.90386665],\n",
       "         [ 1.1941755 ],\n",
       "         [ 1.4742415 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.537873  ],\n",
       "         [-0.5537996 ],\n",
       "         [-0.54471123],\n",
       "         ...,\n",
       "         [ 0.68724954],\n",
       "         [ 0.94733715],\n",
       "         [ 1.2028123 ]],\n",
       " \n",
       "        [[ 0.36068773],\n",
       "         [ 0.28663036],\n",
       "         [ 0.21628958],\n",
       "         ...,\n",
       "         [-3.0887704 ],\n",
       "         [-2.741981  ],\n",
       "         [-2.3768182 ]],\n",
       " \n",
       "        [[-3.6767933 ],\n",
       "         [-4.0569816 ],\n",
       "         [-4.402194  ],\n",
       "         ...,\n",
       "         [ 1.2440798 ],\n",
       "         [ 1.445604  ],\n",
       "         [ 1.6487337 ]]], dtype=float32)>,\n",
       " 'y': <tf.Tensor: shape=(250, 84421, 1), dtype=float32, numpy=\n",
       " array([[[ 8.694591 ],\n",
       "         [11.0146   ],\n",
       "         [10.752121 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 5.750615 ]],\n",
       " \n",
       "        [[ 7.065053 ],\n",
       "         [ 6.1510744],\n",
       "         [ 4.8168774],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 9.158904 ]],\n",
       " \n",
       "        [[ 9.86075  ],\n",
       "         [10.307872 ],\n",
       "         [ 9.393826 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 7.498744 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 8.622354 ],\n",
       "         [ 9.037568 ],\n",
       "         [ 9.269642 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 7.614181 ]],\n",
       " \n",
       "        [[10.172936 ],\n",
       "         [ 9.595305 ],\n",
       "         [ 8.191017 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 3.6794317]],\n",
       " \n",
       "        [[ 7.0848675],\n",
       "         [ 6.116405 ],\n",
       "         [ 6.861412 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 7.4998045]]], dtype=float32)>,\n",
       " 'mean_original': <tf.Tensor: shape=(250, 84421, 1), dtype=float32, numpy=\n",
       " array([[[10.359534 ],\n",
       "         [10.385339 ],\n",
       "         [10.373768 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 6.490466 ]],\n",
       " \n",
       "        [[10.366076 ],\n",
       "         [10.385778 ],\n",
       "         [10.3812685],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 6.4995804]],\n",
       " \n",
       "        [[10.390468 ],\n",
       "         [10.37808  ],\n",
       "         [10.3718605],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 6.4949646]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[10.360427 ],\n",
       "         [10.381398 ],\n",
       "         [10.366727 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 6.500323 ]],\n",
       " \n",
       "        [[ 9.434604 ],\n",
       "         [ 9.420805 ],\n",
       "         [ 9.213946 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 6.228508 ]],\n",
       " \n",
       "        [[10.387343 ],\n",
       "         [10.385359 ],\n",
       "         [10.383357 ],\n",
       "         ...,\n",
       "         [       nan],\n",
       "         [       nan],\n",
       "         [ 6.5033016]]], dtype=float32)>,\n",
       " 'resid': <tf.Tensor: shape=(250, 84421, 1), dtype=float32, numpy=\n",
       " array([[[-1.6649438 ],\n",
       "         [ 0.62926084],\n",
       "         [ 0.3783527 ],\n",
       "         ...,\n",
       "         [-0.1993094 ],\n",
       "         [ 1.4829309 ],\n",
       "         [-0.7398509 ]],\n",
       " \n",
       "        [[-3.3010237 ],\n",
       "         [-4.234704  ],\n",
       "         [-5.564391  ],\n",
       "         ...,\n",
       "         [-0.00632143],\n",
       "         [ 1.2896601 ],\n",
       "         [ 2.6593237 ]],\n",
       " \n",
       "        [[-0.52971756],\n",
       "         [-0.07020816],\n",
       "         [-0.9780345 ],\n",
       "         ...,\n",
       "         [ 0.7593899 ],\n",
       "         [ 0.64994854],\n",
       "         [ 1.0037793 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.7380732 ],\n",
       "         [-1.3438303 ],\n",
       "         [-1.0970845 ],\n",
       "         ...,\n",
       "         [ 1.3930895 ],\n",
       "         [ 0.4508533 ],\n",
       "         [ 1.1138582 ]],\n",
       " \n",
       "        [[ 0.73833245],\n",
       "         [ 0.17450066],\n",
       "         [-1.0229287 ],\n",
       "         ...,\n",
       "         [-1.7105006 ],\n",
       "         [-3.5201385 ],\n",
       "         [-2.5490763 ]],\n",
       " \n",
       "        [[-3.3024762 ],\n",
       "         [-4.268954  ],\n",
       "         [-3.5219452 ],\n",
       "         ...,\n",
       "         [ 0.01379693],\n",
       "         [ 2.01331   ],\n",
       "         [ 0.9965027 ]]], dtype=float32)>}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(means_test_mcmc.shape)\n",
    "bne_joint_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_pred_bne_vo = {k: np.mean(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "surface_var_bne_vo = {k: np.var(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88_VOXlgq9n_"
   },
   "source": [
    "## Bayesian Nonparametric Ensemble (Variance + Skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model & Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior model graph: (('gp_weights', ()), ('y', ('gp_weights',)))\n",
      "Running MAP:\t7174.9990234375...7055.798828125...7053.3818359375...7053.35888671875...7053.3583984375...7053.35888671875...7053.35791015625...7053.35791015625...7053.35791015625...7053.3583984375...Done.\n",
      "Running MCMC:\tAcceptance Ratio: 0.7549411058425903\n"
     ]
    }
   ],
   "source": [
    "# Construct prior distribution.\n",
    "bne_prior, bne_gp_config = bne_model_dist(\n",
    "    inputs=X_train_mcmc,\n",
    "    mean_preds=means_train_mcmc,\n",
    "    **bne_model_config)\n",
    "\n",
    "bne_model_config.update(bne_gp_config)\n",
    "print(f'prior model graph: {bne_prior.resolve_graph()}')\n",
    "# Estimates GP weight posterior using MCMC.\n",
    "bne_gp_w_samples = run_posterior_inference(model_dist=bne_prior,\n",
    "                                           model_config=bne_gp_config,\n",
    "                                           Y=Y_train_mcmc,\n",
    "                                           map_config=map_config,\n",
    "                                           mcmc_config=mcmc_config,\n",
    "                                           initialize_from_map=True)\n",
    "# Generates the posterior sample for all model parameters. \n",
    "bne_joint_samples = make_bne_samples(X_test1,\n",
    "                                     mean_preds=means_test_mcmc,\n",
    "                                     bne_model_config=bne_model_config,\n",
    "                                     bne_weight_samples=bne_gp_w_samples[0],\n",
    "                                     seed=bne_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_pred_bne_vs = {k: np.mean(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}\n",
    "surface_var_bne_vs = {k: np.var(np.nan_to_num(bne_joint_samples[k]), axis=0) for k in ('y', 'mean_original', 'resid')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_DATA_ADDR_PREFIX = \"./example/data\"\n",
    "# Tuning Parameters\n",
    "BMA_lenthscale = bma_gp_lengthscale\n",
    "BNE_lenthscale = bne_gp_lengthscale\n",
    "BMA_L2 = bma_gp_l2_regularizer\n",
    "BNE_L2 = bne_gp_l2_regularizer\n",
    "_SAVE_ADDR_PREFIX = \"./pic/BMA_lenthscale_{}_L2_{}_BNE_lenthscale_{}_L2_{}\".format(BMA_lenthscale, BMA_L2, BNE_lenthscale, BNE_L2)\n",
    "\n",
    "path=_SAVE_ADDR_PREFIX\n",
    "isExists=os.path.exists(path) #判断路径是否存在，存在则返回true\n",
    "\n",
    "if not isExists:\n",
    "    os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The predictive surface of individual base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = np.asarray(base_model_predictions_eastMA[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "monitors = np.asarray(training_eastMA_noMI[[\"lon\", \"lat\"]].values.tolist()).astype(np.float32)\n",
    "base_model_names = [\"pred_av\", \"pred_gs\", \"pred_caces\"]\n",
    "\n",
    "base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]] = np.where(np.isnan(base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]]), 0, base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]])\n",
    "color_norm_base = make_color_norm(\n",
    "    base_model_predictions_eastMA[[\"pred_av\", \"pred_gs\", \"pred_caces\"]],   \n",
    "    method=\"percentile\")\n",
    "\n",
    "for base_model_name in base_model_names:\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                             'base_model_{}_bmals_{}_r_{}_bnels_{}_r_{}.png'.format(\n",
    "                                 base_model_name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                 bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    \n",
    "    posterior_heatmap_2d(base_model_predictions_eastMA[base_model_name], coordinate,\n",
    "                         monitors,\n",
    "                         cmap='RdYlGn_r',\n",
    "                         norm=color_norm_base, \n",
    "                         #norm_method=\"percentile\",\n",
    "                         save_addr=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The predictive surface of individual BNE gp weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bma_ensemble_weights = bma_joint_samples['ensemble_weights']\n",
    "ensemble_weights_val = tf.reduce_mean(bma_ensemble_weights, axis=0)\n",
    "\n",
    "weights_dict = {\n",
    "    \"AV\": ensemble_weights_val[:, 0],\n",
    "    \"GS\": ensemble_weights_val[:,1],\n",
    "    \"CACES\": ensemble_weights_val[:,2],\n",
    "}\n",
    "#weights_dict\n",
    "color_norm_weights = make_color_norm(\n",
    "    list(weights_dict.values()),#[2],   \n",
    "    method=\"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_weights_var = np.var(bma_ensemble_weights, axis=0)\n",
    "weights_var_dict = {\n",
    "    \"AV\": ensemble_weights_var[:, 0],\n",
    "    \"GS\": ensemble_weights_var[:,1],\n",
    "    \"CACES\": ensemble_weights_var[:,2],\n",
    "}\n",
    "#weights_dict\n",
    "color_norm_weights_var = make_color_norm(\n",
    "    list(weights_var_dict.values()),#[0],   \n",
    "    method=\"percentile\")\n",
    "# display(ensemble_weights_val,ensemble_weights_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_names = [\"AV\", \"GS\", \"CACES\"]\n",
    "for base_model_name in base_model_names:\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                             'base_weights_{}_bmals_{}_r_{}_bnels_{}_r_{}.png'.format(\n",
    "                                 base_model_name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                 bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    \n",
    "    posterior_heatmap_2d(weights_dict[base_model_name], coordinate,\n",
    "                         monitors,\n",
    "                         cmap='viridis',\n",
    "                         norm=color_norm_weights, \n",
    "                         #norm_method=\"percentile\",\n",
    "                         #save_addr='')\n",
    "                         save_addr=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weights' variance\n",
    "for base_model_name in base_model_names:\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                             'base_wvar_{}_bmals_{}_r_{}_bnels_{}_r_{}.png'.format(\n",
    "                                 base_model_name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                 bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    \n",
    "    posterior_heatmap_2d(weights_var_dict[base_model_name], coordinate,\n",
    "                         monitors,\n",
    "                         cmap='viridis',\n",
    "                         norm=color_norm_weights_var, \n",
    "                         #norm_method=\"percentile\",\n",
    "                         #save_addr='')\n",
    "                         save_addr=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The predictive surface of Y_mean, residual process, and Y_mean + residual process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BAE\n",
    "color_norm_pred = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[:2][0]),\n",
    "    list(surface_pred_bae.values())[:2],  \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_pred_r = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[2:]),\n",
    "    list(surface_pred_bae.values())[2],  \n",
    "    method=\"residual_percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_pred_bae.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'BAE_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "\n",
    "    value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                                  cmap='RdYlGn_r',\n",
    "                    norm= color_norm_pred_r if name=='resid' else color_norm_pred,\n",
    "                    #norm_method=\"percentile\",\n",
    "                    #save_addr='')\n",
    "                    save_addr=save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE vo\n",
    "color_norm_pred = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[:2][0]),\n",
    "    list(surface_pred_bne_vo.values())[:2],  \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_pred_r = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[2:]),\n",
    "    list(surface_pred_bne_vo.values())[2],  \n",
    "    method=\"residual_percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_pred_bne_vo.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'BNEvo_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "\n",
    "    value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                                  cmap='RdYlGn_r',\n",
    "                    norm= color_norm_pred_r if name=='resid' else color_norm_pred,\n",
    "                                      save_addr=save_name)\n",
    "                    #norm_method=\"percentile\",\n",
    "                    #save_addr='')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE v+s\n",
    "color_norm_pred = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[:2][0]),\n",
    "    list(surface_pred_bne_vs.values())[:2],  \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_pred_r = make_color_norm(\n",
    "    #np.nan_to_num(list(surface_pred_bae.values())[2:]),\n",
    "    list(surface_pred_bne_vs.values())[2],  \n",
    "    method=\"residual_percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_pred_bne_vs.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'BNEvs_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(name, bma_gp_lengthscale, \n",
    "                                bma_gp_l2_regularizer, bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "\n",
    "    value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                                  cmap='RdYlGn_r',\n",
    "                    norm= color_norm_pred_r if name=='resid' else color_norm_pred,\n",
    "                    #norm_method=\"percentile\",\n",
    "                    #save_addr='')\n",
    "                save_addr=save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.The predictive variance of Y_mean, residual process, and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAE\n",
    "color_norm_var = make_color_norm(\n",
    "    list(surface_var_bae.values())[:2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_var_r = make_color_norm(\n",
    "    list(surface_var_bae.values())[2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_var_bae.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'var_BAE_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    #value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                cmap='inferno_r',\n",
    "                                norm= color_norm_var_r if name=='resid' else color_norm_var,\n",
    "                                #norm_method=\"percentile\",\n",
    "                                save_addr=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE vo\n",
    "color_norm_var = make_color_norm(\n",
    "    list(surface_var_bne_vo.values())[:2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_var_r = make_color_norm(\n",
    "    list(surface_var_bne_vo.values())[2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_var_bne_vo.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'var_BNEvo_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    #value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                cmap='inferno_r',\n",
    "                                norm= color_norm_var_r if name=='resid' else color_norm_var,\n",
    "                                #norm_method=\"percentile\",\n",
    "                                save_addr=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNE v+s\n",
    "color_norm_var = make_color_norm(\n",
    "    list(surface_var_bne_vs.values())[:2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "color_norm_var_r = make_color_norm(\n",
    "    list(surface_var_bne_vs.values())[2], \n",
    "    method=\"percentile\")\n",
    "\n",
    "\n",
    "for name, value in surface_var_bne_vs.items():\n",
    "    save_name = os.path.join(_SAVE_ADDR_PREFIX,\n",
    "                            'var_BNEvs_{}_bma:ls_{}_r_{}_bne:ls_{}_r_{}.png'.format(\n",
    "                                name, bma_gp_lengthscale,  bma_gp_l2_regularizer,\n",
    "                                bne_gp_lengthscale, bne_gp_l2_regularizer))\n",
    "    #value = np.where(np.isnan(value), 0, value)\n",
    "    color_norm = posterior_heatmap_2d(value, X=coordinate, X_monitor=monitors,\n",
    "                                cmap='inferno_r',\n",
    "                                norm= color_norm_var_r if name=='resid' else color_norm_var,\n",
    "                                #norm_method=\"percentile\",\n",
    "                                save_addr=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWSzplhQKdqt"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Simulation: compute_metrics\n",
    "def compute_metrics(data_dict, q_true=None, ind_ids=None, num_sample=None):\n",
    "  if q_true is None:\n",
    "    q_true = np.array(\n",
    "        [0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25,\n",
    "         0.75, 0.8, 0.85, 0.9, 0.925, 0.95, 0.975])\n",
    "\n",
    "  if ind_ids is None:\n",
    "    # Find IDs of in-domain test data via range comparison \n",
    "    # between X_train and X_test.\n",
    "    X_train_min = np.min(data_dict['X_train'], axis=0)\n",
    "    X_train_max = np.max(data_dict['X_train'], axis=0)\n",
    "\n",
    "    test_ids_greater_than_min = np.all(\n",
    "        data_dict['X_test'] > X_train_min, axis=-1)\n",
    "    test_ids_less_than_max = np.all(\n",
    "        data_dict['X_test'] < X_train_max, axis=-1)\n",
    "\n",
    "    ind_ids = np.where(\n",
    "        np.logical_and(test_ids_greater_than_min, test_ids_less_than_max))[0]\n",
    "\n",
    "  samples = data_dict[f'{model_name}_samples']\n",
    "  means_true = data_dict['mean_test']\n",
    "  y_test = data_dict['Y_test']\n",
    "\n",
    "  if num_sample is not None:\n",
    "    samples = samples[:num_sample]\n",
    "\n",
    "  means_pred = np.mean(samples, axis=0)\n",
    "  stds_pred = np.std(samples, axis=0)\n",
    "  quantile_pred = np.quantile(samples, q=q_true, axis=0)\n",
    "\n",
    "  # Compute in-domain metrics.\n",
    "  nll_ind = np.mean(\n",
    "      ((means_pred[ind_ids] - means_true[ind_ids])/stds_pred[ind_ids])**2 + \n",
    "      np.log(stds_pred[ind_ids]))\n",
    "  clb_ind = np.mean(\n",
    "      ((means_pred[ind_ids] - means_true[ind_ids])/stds_pred[ind_ids])**2)\n",
    "  shp_ind = np.mean(np.log(stds_pred[ind_ids]))\n",
    "  mse_ind = np.mean(\n",
    "      (means_pred[ind_ids] - means_true[ind_ids])**2) / np.var(means_true[ind_ids])\n",
    "\n",
    "  q_pred_ind = np.mean(y_test[ind_ids] < quantile_pred[:, ind_ids], axis=(1, 2))\n",
    "  ece_ind = np.mean((q_pred_ind - q_true)**2)\n",
    "  cov_prob_95_ind = q_pred_ind[-1] - q_pred_ind[0]\n",
    "  cov_prob_90_ind = q_pred_ind[-2] - q_pred_ind[1]\n",
    "  cov_prob_85_ind = q_pred_ind[-3] - q_pred_ind[2]\n",
    "  cov_prob_80_ind = q_pred_ind[-4] - q_pred_ind[3]\n",
    "\n",
    "  # Compute all-domain (ind + ood) metrics.\n",
    "  nll_all = np.mean(((means_pred - means_true)/stds_pred)**2 + np.log(stds_pred))\n",
    "  clb_all = np.mean(((means_pred - means_true)/stds_pred)**2)\n",
    "  shp_all = np.mean(np.log(stds_pred))\n",
    "  mse_all = np.mean((means_pred - means_true)**2) / np.var(means_true)\n",
    "\n",
    "  q_pred_all = np.mean(y_test < quantile_pred, axis=(1, 2))\n",
    "  ece_all = np.mean((q_pred_all - q_true)**2)\n",
    "  cov_prob_95_all = q_pred_all[-1] - q_pred_all[0]\n",
    "  cov_prob_90_all = q_pred_all[-2] - q_pred_all[1]\n",
    "  cov_prob_85_all = q_pred_all[-3] - q_pred_all[2]\n",
    "  cov_prob_80_all = q_pred_all[-4] - q_pred_all[3]\n",
    "\n",
    "  return (mse_ind, nll_ind, clb_ind, shp_ind, ece_ind, cov_prob_95_ind, cov_prob_90_ind, cov_prob_85_ind, cov_prob_80_ind,\n",
    "          mse_all, nll_all, clb_all, shp_all, ece_all, cov_prob_95_all, cov_prob_90_all, cov_prob_85_all, cov_prob_80_all)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FyVOAW4EODnT",
    "ebzyBOEoNQ_a",
    "vAgjEq1-dty-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('BNE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fcc6cf470fa43d25f7728c2f3c746fc9b5580c34ff527761ed8536047e15184c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
